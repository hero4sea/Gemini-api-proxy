import asyncio
import json
import time
import uuid
import logging
import os
import sys
import base64
import mimetypes
from datetime import datetime
from typing import Dict, List, Optional, AsyncGenerator, Union, Any
from contextlib import asynccontextmanager

import httpx
import uvicorn
from fastapi import FastAPI, HTTPException, Request, Header, File, UploadFile, Form
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.exceptions import RequestValidationError
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, ValidationError, validator
from apscheduler.schedulers.asyncio import AsyncIOScheduler

from database import Database

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡
start_time = time.time()
request_count = 0


# æ€è€ƒé…ç½®æ¨¡å‹
class ThinkingConfig(BaseModel):
    thinking_budget: Optional[int] = None  # 0-32768, 0=ç¦ç”¨æ€è€ƒ, None=è‡ªåŠ¨
    include_thoughts: Optional[bool] = False  # æ˜¯å¦åœ¨å“åº”ä¸­åŒ…å«æ€è€ƒè¿‡ç¨‹

    class Config:
        extra = "allow"

    @validator('thinking_budget')
    def validate_thinking_budget(cls, v):
        if v is not None:
            if not isinstance(v, int) or v < 0 or v > 32768:
                raise ValueError("thinking_budget must be an integer between 0 and 32768")
        return v


# æ–‡ä»¶æ•°æ®æ¨¡å‹
class FileData(BaseModel):
    file_id: str
    mime_type: str
    data: Optional[str] = None  # base64ç¼–ç çš„æ–‡ä»¶æ•°æ®ï¼ˆå°æ–‡ä»¶ï¼‰
    file_uri: Optional[str] = None  # æ–‡ä»¶URIï¼ˆå¤§æ–‡ä»¶ï¼‰
    size: Optional[int] = None
    filename: Optional[str] = None


# å¤šæ¨¡æ€å†…å®¹éƒ¨åˆ†
class ContentPart(BaseModel):
    type: str  # "text", "image", "audio", "video", "document"
    text: Optional[str] = None
    file_data: Optional[FileData] = None


# è¯·æ±‚/å“åº”æ¨¡å‹
class ChatMessage(BaseModel):
    role: str
    content: Union[str, List[Union[str, Dict[str, Any], ContentPart]]]  # æ”¯æŒå¤šæ¨¡æ€å†…å®¹

    class Config:
        # å…è®¸é¢å¤–å­—æ®µï¼Œæé«˜å…¼å®¹æ€§
        extra = "allow"

    @validator('content')
    def validate_content(cls, v):
        """éªŒè¯å¹¶æ ‡å‡†åŒ–contentå­—æ®µ"""
        if isinstance(v, str):
            return v
        elif isinstance(v, list):
            # ä¿æŒåŸå§‹æ ¼å¼ä»¥æ”¯æŒå¤šæ¨¡æ€
            return v
        else:
            raise ValueError("content must be string or array of content objects")

    def get_text_content(self) -> str:
        """è·å–çº¯æ–‡æœ¬å†…å®¹"""
        if isinstance(self.content, str):
            return self.content
        elif isinstance(self.content, list):
            text_parts = []
            for item in self.content:
                if isinstance(item, str):
                    text_parts.append(item)
                elif isinstance(item, dict):
                    if item.get('type') == 'text' and 'text' in item:
                        text_parts.append(item['text'])
                    elif 'text' in item:
                        text_parts.append(item['text'])
            return ' '.join(text_parts) if text_parts else ""
        else:
            return str(self.content)

    def has_multimodal_content(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«å¤šæ¨¡æ€å†…å®¹"""
        if isinstance(self.content, list):
            for item in self.content:
                if isinstance(item, dict) and item.get('type') in ['image', 'audio', 'video', 'document']:
                    return True
        return False


class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    temperature: Optional[float] = 1.0
    top_p: Optional[float] = 1.0
    n: Optional[int] = 1
    stream: Optional[bool] = False
    stop: Optional[List[str]] = None
    max_tokens: Optional[int] = None
    presence_penalty: Optional[float] = 0
    frequency_penalty: Optional[float] = 0
    user: Optional[str] = None
    # æ€è€ƒé…ç½®
    thinking_config: Optional[ThinkingConfig] = None

    class Config:
        # å…è®¸é¢å¤–å­—æ®µï¼Œæé«˜ä¸OpenAI SDKçš„å…¼å®¹æ€§
        extra = "allow"

    # è‡ªå®šä¹‰éªŒè¯å™¨ï¼Œç¡®ä¿å‚æ•°åœ¨åˆç†èŒƒå›´å†…
    def __init__(self, **data):
        # å‚æ•°èŒƒå›´éªŒè¯
        if 'temperature' in data and data['temperature'] is not None:
            data['temperature'] = max(0.0, min(2.0, data['temperature']))
        if 'top_p' in data and data['top_p'] is not None:
            data['top_p'] = max(0.0, min(1.0, data['top_p']))
        if 'n' in data and data['n'] is not None:
            data['n'] = max(1, min(4, data['n']))
        if 'max_tokens' in data and data['max_tokens'] is not None:
            data['max_tokens'] = max(1, min(8192, data['max_tokens']))

        super().__init__(**data)


# å†…å­˜ç¼“å­˜ç”¨äºRPM/TPMé™åˆ¶
class RateLimitCache:
    def __init__(self, max_entries: int = 10000):
        self.cache: Dict[str, Dict[str, List[tuple]]] = {}
        self.max_entries = max_entries
        self.lock = asyncio.Lock()

    async def cleanup_expired(self, window_seconds: int = 60):
        """å®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜"""
        current_time = time.time()
        cutoff_time = current_time - window_seconds

        async with self.lock:
            for model_name in list(self.cache.keys()):
                if model_name in self.cache:
                    self.cache[model_name]['requests'] = [
                        (t, v) for t, v in self.cache[model_name]['requests']
                        if t > cutoff_time
                    ]
                    self.cache[model_name]['tokens'] = [
                        (t, v) for t, v in self.cache[model_name]['tokens']
                        if t > cutoff_time
                    ]

    async def add_usage(self, model_name: str, requests: int = 1, tokens: int = 0):
        async with self.lock:
            if model_name not in self.cache:
                self.cache[model_name] = {'requests': [], 'tokens': []}

            current_time = time.time()
            self.cache[model_name]['requests'].append((current_time, requests))
            self.cache[model_name]['tokens'].append((current_time, tokens))

    async def get_current_usage(self, model_name: str, window_seconds: int = 60) -> Dict[str, int]:
        async with self.lock:
            if model_name not in self.cache:
                return {'requests': 0, 'tokens': 0}

            current_time = time.time()
            cutoff_time = current_time - window_seconds

            # æ¸…ç†è¿‡æœŸè®°å½•
            self.cache[model_name]['requests'] = [
                (t, v) for t, v in self.cache[model_name]['requests']
                if t > cutoff_time
            ]
            self.cache[model_name]['tokens'] = [
                (t, v) for t, v in self.cache[model_name]['tokens']
                if t > cutoff_time
            ]

            # è®¡ç®—æ€»å’Œ
            total_requests = sum(v for _, v in self.cache[model_name]['requests'])
            total_tokens = sum(v for _, v in self.cache[model_name]['tokens'])

            return {'requests': total_requests, 'tokens': total_tokens}


# ä¿æŒå”¤é†’æœºåˆ¶ï¼ˆä»…åœ¨Renderç¯å¢ƒå¯ç”¨ï¼‰
async def keep_alive():
    """ä¿æŒæœåŠ¡å”¤é†’"""
    try:
        render_url = os.getenv('RENDER_EXTERNAL_URL')
        if render_url:
            async with httpx.AsyncClient(timeout=10) as client:
                await client.get(f"{render_url}/wake")
                logger.info("Keep-alive ping sent successfully")
    except Exception as e:
        logger.warning(f"Keep-alive ping failed: {e}")


# å¥åº·æ£€æµ‹åŠŸèƒ½
async def check_gemini_key_health(api_key: str, timeout: int = 10) -> Dict[str, Any]:
    """æ£€æµ‹å•ä¸ªGemini Keyçš„å¥åº·çŠ¶æ€"""
    test_request = {
        "contents": [{"role": "user", "parts": [{"text": "Test"}]}],
        "generationConfig": {"maxOutputTokens": 1}
    }

    start_time = time.time()
    try:
        async with httpx.AsyncClient(timeout=timeout) as client:
            response = await client.post(
                "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent",
                json=test_request,
                headers={"x-goog-api-key": api_key}
            )

        response_time = time.time() - start_time

        if response.status_code == 200:
            return {
                "healthy": True,
                "response_time": response_time,
                "status_code": response.status_code,
                "error": None
            }
        else:
            return {
                "healthy": False,
                "response_time": response_time,
                "status_code": response.status_code,
                "error": f"HTTP {response.status_code}"
            }

    except asyncio.TimeoutError:
        return {
            "healthy": False,
            "response_time": timeout,
            "status_code": None,
            "error": "Timeout"
        }
    except Exception as e:
        return {
            "healthy": False,
            "response_time": time.time() - start_time,
            "status_code": None,
            "error": str(e)
        }


# å…¨å±€å˜é‡
db = Database()
rate_limiter = RateLimitCache()
scheduler = None

# æ–‡ä»¶å­˜å‚¨é…ç½®
UPLOAD_DIR = "uploads"
MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB
SUPPORTED_MIME_TYPES = {
    # å›¾ç‰‡
    'image/jpeg', 'image/png', 'image/gif', 'image/webp', 'image/bmp',
    # éŸ³é¢‘
    'audio/mpeg', 'audio/wav', 'audio/ogg', 'audio/mp4', 'audio/flac',
    # è§†é¢‘
    'video/mp4', 'video/avi', 'video/mov', 'video/wmv', 'video/webm',
    # æ–‡æ¡£
    'application/pdf', 'text/plain', 'text/csv',
    'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
    'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
}

# ç¡®ä¿ä¸Šä¼ ç›®å½•å­˜åœ¨
os.makedirs(UPLOAD_DIR, exist_ok=True)

# æ–‡ä»¶å­˜å‚¨å­—å…¸ï¼ˆå†…å­˜å­˜å‚¨ï¼Œç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨æ•°æ®åº“ï¼‰
file_storage: Dict[str, Dict] = {}


@asynccontextmanager
async def lifespan(app: FastAPI):
    global scheduler
    # å¯åŠ¨æ—¶çš„æ“ä½œ
    logger.info("Starting Gemini API Proxy...")
    logger.info(f"Available API keys: {len(db.get_available_gemini_keys())}")
    logger.info(f"Environment: {'Render' if os.getenv('RENDER_EXTERNAL_URL') else 'Local'}")

    # å¯åŠ¨ä¿æŒå”¤é†’è°ƒåº¦å™¨ï¼ˆä»…åœ¨Renderç¯å¢ƒï¼‰
    render_url = os.getenv('RENDER_EXTERNAL_URL')
    if render_url:
        scheduler = AsyncIOScheduler()
        scheduler.add_job(
            keep_alive,
            'interval',
            minutes=14,  # åœ¨15åˆ†é’Ÿç¡çœ å‰ä¿æŒå”¤é†’
            id='keep_alive',
            max_instances=1
        )
        scheduler.start()
        logger.info("Keep-alive scheduler started (14min interval)")

    yield

    # å…³é—­æ—¶çš„æ“ä½œ
    if scheduler:
        scheduler.shutdown()
        logger.info("Scheduler shutdown")
    logger.info("API Server shutting down...")


app = FastAPI(
    title="Gemini API Proxy",
    description="A high-performance proxy for Gemini API with OpenAI compatibility",
    version="1.1.0",
    lifespan=lifespan
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒåº”é™åˆ¶å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# è¯·æ±‚è®¡æ•°ä¸­é—´ä»¶
@app.middleware("http")
async def count_requests(request: Request, call_next):
    global request_count
    request_count += 1

    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time

    # è®°å½•è¯·æ±‚æ—¥å¿—
    logger.info(f"{request.method} {request.url.path} - {response.status_code} - {process_time:.3f}s")

    return response


# å…¨å±€å¼‚å¸¸å¤„ç†å™¨
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    """å¤„ç†è¯·æ±‚éªŒè¯é”™è¯¯"""
    logger.warning(f"Request validation error: {exc}")

    # æå–å…·ä½“çš„é”™è¯¯ä¿¡æ¯
    error_details = []
    for error in exc.errors():
        field = " -> ".join(str(x) for x in error["loc"])
        msg = error["msg"]
        error_details.append(f"{field}: {msg}")

    return JSONResponse(
        status_code=422,
        content={
            "error": {
                "message": f"Request validation failed: {'; '.join(error_details)}",
                "type": "invalid_request_error",
                "code": "request_validation_error"
            }
        }
    )


@app.exception_handler(ValidationError)
async def pydantic_validation_exception_handler(request: Request, exc: ValidationError):
    """å¤„ç†PydanticéªŒè¯é”™è¯¯"""
    logger.warning(f"Pydantic validation error: {exc}")

    return JSONResponse(
        status_code=422,
        content={
            "error": {
                "message": f"Data validation failed: {str(exc)}",
                "type": "invalid_request_error",
                "code": "data_validation_error"
            }
        }
    )


@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """å…¨å±€å¼‚å¸¸å¤„ç†"""
    logger.error(f"Global exception: {str(exc)}")
    return JSONResponse(
        status_code=500,
        content={
            "error": {
                "message": "Internal server error",
                "type": "internal_error",
                "code": "server_error"
            }
        }
    )


# è¾…åŠ©å‡½æ•°
def get_actual_model_name(request_model: str) -> str:
    """è·å–å®é™…ä½¿ç”¨çš„æ¨¡å‹åç§°"""
    supported_models = db.get_supported_models()

    # å¦‚æœè¯·æ±‚çš„æ˜¯æ”¯æŒçš„æ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨
    if request_model in supported_models:
        logger.info(f"Using requested model: {request_model}")
        return request_model

    # å¦åˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
    default_model = db.get_config('default_model_name', 'gemini-2.5-flash')
    logger.info(f"Unsupported model: {request_model}, using default: {default_model}")
    return default_model


def inject_prompt_to_messages(messages: List[ChatMessage]) -> List[ChatMessage]:
    """å‘æ¶ˆæ¯ä¸­æ³¨å…¥prompt"""
    inject_config = db.get_inject_prompt_config()

    if not inject_config['enabled'] or not inject_config['content']:
        return messages

    content = inject_config['content']
    position = inject_config['position']

    # åˆ›å»ºæ¶ˆæ¯å‰¯æœ¬
    new_messages = messages.copy()

    if position == 'system':
        # æ·»åŠ æˆ–æ›´æ–°systemæ¶ˆæ¯
        system_msg = None
        for i, msg in enumerate(new_messages):
            if msg.role == 'system':
                system_msg = msg
                break

        if system_msg:
            # æ›´æ–°ç°æœ‰systemæ¶ˆæ¯
            new_content = f"{content}\n\n{system_msg.get_text_content()}"
            new_messages[i] = ChatMessage(role='system', content=new_content)
        else:
            # æ·»åŠ æ–°çš„systemæ¶ˆæ¯åˆ°å¼€å¤´
            new_messages.insert(0, ChatMessage(role='system', content=content))

    elif position == 'user_prefix':
        # åœ¨ç¬¬ä¸€ä¸ªuseræ¶ˆæ¯å‰æ·»åŠ 
        for i, msg in enumerate(new_messages):
            if msg.role == 'user':
                original_content = msg.get_text_content()
                new_content = f"{content}\n\n{original_content}"
                new_messages[i] = ChatMessage(role='user', content=new_content)
                break

    elif position == 'user_suffix':
        # åœ¨æœ€åä¸€ä¸ªuseræ¶ˆæ¯åæ·»åŠ 
        for i in range(len(new_messages) - 1, -1, -1):
            if new_messages[i].role == 'user':
                original_content = new_messages[i].get_text_content()
                new_content = f"{original_content}\n\n{content}"
                new_messages[i] = ChatMessage(role='user', content=new_content)
                break

    return new_messages


def get_thinking_config(request: ChatCompletionRequest) -> Dict:
    """æ ¹æ®é…ç½®ç”Ÿæˆæ€è€ƒé…ç½®"""
    thinking_config = {}

    # ä»æ•°æ®åº“è·å–å…¨å±€é…ç½®
    global_thinking_enabled = db.get_config('thinking_enabled', 'true').lower() == 'true'
    global_thinking_budget = int(db.get_config('thinking_budget', '-1'))  # -1 è¡¨ç¤ºè‡ªåŠ¨
    global_include_thoughts = db.get_config('include_thoughts', 'false').lower() == 'true'

    # å¦‚æœå…¨å±€ç¦ç”¨æ€è€ƒï¼Œç›´æ¥è¿”å›ç¦ç”¨é…ç½®
    if not global_thinking_enabled:
        return {"thinkingBudget": 0}

    # å¦‚æœè¯·æ±‚ä¸­æœ‰æ€è€ƒé…ç½®ï¼Œä¼˜å…ˆä½¿ç”¨è¯·æ±‚çš„é…ç½®
    if request.thinking_config:
        if request.thinking_config.thinking_budget is not None:
            thinking_config["thinkingBudget"] = request.thinking_config.thinking_budget
        elif global_thinking_budget >= 0:
            thinking_config["thinkingBudget"] = global_thinking_budget

        if request.thinking_config.include_thoughts is not None:
            thinking_config["includeThoughts"] = request.thinking_config.include_thoughts
        elif global_include_thoughts:
            thinking_config["includeThoughts"] = global_include_thoughts
    else:
        # ä½¿ç”¨å…¨å±€é…ç½®
        if global_thinking_budget >= 0:
            thinking_config["thinkingBudget"] = global_thinking_budget
        if global_include_thoughts:
            thinking_config["includeThoughts"] = global_include_thoughts

    return thinking_config


def openai_to_gemini(request: ChatCompletionRequest) -> Dict:
    """å°†OpenAIæ ¼å¼è½¬æ¢ä¸ºGeminiæ ¼å¼ï¼Œæ”¯æŒå¤šæ¨¡æ€å†…å®¹"""
    contents = []

    for msg in request.messages:
        parts = []

        if isinstance(msg.content, str):
            # çº¯æ–‡æœ¬æ¶ˆæ¯
            if msg.role == "system":
                parts.append({"text": f"[System]: {msg.content}"})
            else:
                parts.append({"text": msg.content})
        elif isinstance(msg.content, list):
            # å¤šæ¨¡æ€æ¶ˆæ¯
            for item in msg.content:
                if isinstance(item, str):
                    parts.append({"text": item})
                elif isinstance(item, dict):
                    if item.get('type') == 'text':
                        parts.append({"text": item.get('text', '')})
                    elif item.get('type') in ['image', 'audio', 'video', 'document']:
                        # å¤„ç†æ–‡ä»¶å†…å®¹
                        file_data = item.get('file_data')
                        if file_data:
                            if file_data.get('data'):  # å°æ–‡ä»¶ï¼Œä½¿ç”¨å†…è”æ•°æ®
                                parts.append({
                                    "inlineData": {
                                        "mimeType": file_data['mime_type'],
                                        "data": file_data['data']
                                    }
                                })
                            elif file_data.get('file_uri'):  # å¤§æ–‡ä»¶ï¼Œä½¿ç”¨æ–‡ä»¶URI
                                parts.append({
                                    "fileData": {
                                        "mimeType": file_data['mime_type'],
                                        "fileUri": file_data['file_uri']
                                    }
                                })

        # ç¡®å®šè§’è‰²
        role = "user" if msg.role in ["system", "user"] else "model"

        if parts:  # åªæœ‰å½“æœ‰å†…å®¹æ—¶æ‰æ·»åŠ 
            contents.append({
                "role": role,
                "parts": parts
            })

    # æ„å»ºåŸºæœ¬è¯·æ±‚
    gemini_request = {
        "contents": contents,
        "generationConfig": {
            "temperature": request.temperature,
            "topP": request.top_p,
            "candidateCount": request.n,
        }
    }

    # æ·»åŠ æ€è€ƒé…ç½®
    thinking_config = get_thinking_config(request)
    if thinking_config:
        gemini_request["generationConfig"]["thinkingConfig"] = thinking_config

    # æ·»åŠ å…¶ä»–å‚æ•°
    if request.max_tokens:
        gemini_request["generationConfig"]["maxOutputTokens"] = request.max_tokens

    if request.stop:
        gemini_request["generationConfig"]["stopSequences"] = request.stop

    return gemini_request


def extract_thoughts_and_content(gemini_response: Dict) -> tuple[str, str]:
    """ä»Geminiå“åº”ä¸­æå–æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆå†…å®¹"""
    thoughts = ""
    content = ""

    for candidate in gemini_response.get("candidates", []):
        parts = candidate.get("content", {}).get("parts", [])

        for part in parts:
            if "text" in part:
                # æ£€æŸ¥æ˜¯å¦ä¸ºæ€è€ƒéƒ¨åˆ†
                if part.get("thought", False):
                    thoughts += part["text"]
                else:
                    content += part["text"]

    return thoughts, content


def gemini_to_openai(gemini_response: Dict, request: ChatCompletionRequest, usage_info: Dict = None) -> Dict:
    """å°†Geminiå“åº”è½¬æ¢ä¸ºOpenAIæ ¼å¼"""
    choices = []

    # æå–æ€è€ƒè¿‡ç¨‹å’Œå†…å®¹
    thoughts, content = extract_thoughts_and_content(gemini_response)

    for i, candidate in enumerate(gemini_response.get("candidates", [])):
        # æ„å»ºå“åº”æ¶ˆæ¯
        message_content = content if content else ""

        # å¦‚æœæœ‰æ€è€ƒè¿‡ç¨‹ä¸”é…ç½®è¦æ±‚åŒ…å«ï¼Œæ·»åŠ åˆ°å“åº”ä¸­
        if thoughts and request.thinking_config and request.thinking_config.include_thoughts:
            message_content = f"**Thinking:**\n{thoughts}\n\n**Response:**\n{content}"

        choices.append({
            "index": i,
            "message": {
                "role": "assistant",
                "content": message_content
            },
            "finish_reason": map_finish_reason(candidate.get("finishReason", "STOP"))
        })

    # æ„å»ºå“åº”
    response = {
        "id": f"chatcmpl-{uuid.uuid4().hex[:8]}",
        "object": "chat.completion",
        "created": int(time.time()),
        "model": request.model,
        "choices": choices,
        "usage": usage_info or {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0
        }
    }

    return response


def map_finish_reason(gemini_reason: str) -> str:
    """æ˜ å°„Geminiçš„ç»“æŸåŸå› åˆ°OpenAIæ ¼å¼"""
    mapping = {
        "STOP": "stop",
        "MAX_TOKENS": "length",
        "SAFETY": "content_filter",
        "RECITATION": "content_filter",
        "OTHER": "stop"
    }
    return mapping.get(gemini_reason, "stop")


async def select_gemini_key_and_check_limits(model_name: str, excluded_keys: set = None) -> Optional[Dict]:
    """è‡ªé€‚åº”é€‰æ‹©å¯ç”¨çš„Gemini Keyå¹¶æ£€æŸ¥æ¨¡å‹é™åˆ¶ï¼ˆæ”¯æŒæ’é™¤ç‰¹å®škeyï¼‰"""
    if excluded_keys is None:
        excluded_keys = set()

    available_keys = db.get_available_gemini_keys()

    # è¿‡æ»¤æ’é™¤çš„key
    available_keys = [k for k in available_keys if k['id'] not in excluded_keys]

    if not available_keys:
        logger.warning("No available Gemini keys found after exclusions")
        return None

    # è·å–æ¨¡å‹é…ç½®ï¼ˆå·²ç»åŒ…å«è®¡ç®—çš„æ€»é™åˆ¶ï¼‰
    model_config = db.get_model_config(model_name)
    if not model_config:
        logger.error(f"Model config not found for: {model_name}")
        return None

    # è®°å½•é™åˆ¶ä¿¡æ¯
    logger.info(
        f"Model {model_name} limits: RPM={model_config['total_rpm_limit']}, TPM={model_config['total_tpm_limit']}, RPD={model_config['total_rpd_limit']}")
    logger.info(f"Available API keys: {len(available_keys)}")

    # æ£€æŸ¥æ¨¡å‹çº§åˆ«çš„é™åˆ¶ï¼ˆä½¿ç”¨æ€»é™åˆ¶ï¼‰
    current_usage = await rate_limiter.get_current_usage(model_name)

    if (current_usage['requests'] >= model_config['total_rpm_limit'] or
            current_usage['tokens'] >= model_config['total_tpm_limit']):
        logger.warning(
            f"Model {model_name} has reached rate limits: requests={current_usage['requests']}/{model_config['total_rpm_limit']}, tokens={current_usage['tokens']}/{model_config['total_tpm_limit']}")
        return None

    # æ£€æŸ¥RPDé™åˆ¶ï¼ˆä½¿ç”¨æ€»é™åˆ¶ï¼‰
    day_usage = db.get_usage_stats(model_name, 'day')
    if day_usage['requests'] >= model_config['total_rpd_limit']:
        logger.warning(
            f"Model {model_name} has reached daily request limit: {day_usage['requests']}/{model_config['total_rpd_limit']}")
        return None

    # è‡ªé€‚åº”ç­–ç•¥é€‰æ‹©Key
    strategy = db.get_config('load_balance_strategy', 'adaptive')

    if strategy == 'round_robin':
        # ç®€å•è½®è¯¢ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„
        selected_key = available_keys[0]
    elif strategy == 'least_used':
        # é€‰æ‹©ä½¿ç”¨é‡æœ€å°‘çš„Keyï¼ˆåŸºäºKey IDçš„ä½¿ç”¨åˆ†å¸ƒï¼‰
        selected_key = available_keys[0]
    else:  # adaptive strategy
        # è‡ªé€‚åº”ç­–ç•¥ï¼šç»¼åˆè€ƒè™‘æˆåŠŸç‡ã€å“åº”æ—¶é—´ã€ä½¿ç”¨ç‡
        best_key = None
        best_score = -1

        for key_info in available_keys:
            # è®¡ç®—ç»¼åˆå¾—åˆ†
            success_rate = key_info.get('success_rate', 1.0)
            avg_response_time = key_info.get('avg_response_time', 0.0)

            # å“åº”æ—¶é—´å¾—åˆ†ï¼ˆè¶Šä½è¶Šå¥½ï¼Œè½¬æ¢ä¸º0-1åˆ†æ•°ï¼‰
            time_score = max(0, 1.0 - (avg_response_time / 10.0))  # å‡è®¾10ç§’ä¸ºæœ€å·®å“åº”æ—¶é—´

            # ç»¼åˆå¾—åˆ†ï¼šæˆåŠŸç‡æƒé‡0.7ï¼Œå“åº”æ—¶é—´æƒé‡0.3
            score = success_rate * 0.7 + time_score * 0.3

            if score > best_score:
                best_score = score
                best_key = key_info

        selected_key = best_key if best_key else available_keys[0]

    logger.info(f"Selected API key #{selected_key['id']} for model {model_name} (strategy: {strategy})")

    # è¿”å›é€‰ä¸­çš„Keyå’Œæ¨¡å‹é…ç½®
    return {
        'key_info': selected_key,
        'model_config': model_config
    }


async def make_gemini_request_with_retry(
        gemini_key: str,
        key_id: int,
        gemini_request: Dict,
        model_name: str,
        max_retries: int = 3
) -> Dict:
    """å¸¦é‡è¯•çš„Gemini APIè¯·æ±‚ï¼Œè®°å½•æ€§èƒ½æŒ‡æ ‡"""
    timeout = float(db.get_config('request_timeout', '60'))

    for attempt in range(max_retries):
        start_time = time.time()
        try:
            async with httpx.AsyncClient(timeout=timeout) as client:
                gemini_url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:generateContent"

                response = await client.post(
                    gemini_url,
                    json=gemini_request,
                    headers={"x-goog-api-key": gemini_key}
                )

                response_time = time.time() - start_time

                if response.status_code == 200:
                    # è®°å½•æˆåŠŸçš„æ€§èƒ½æŒ‡æ ‡
                    db.update_key_performance(key_id, True, response_time)
                    return response.json()
                else:
                    # è®°å½•å¤±è´¥çš„æ€§èƒ½æŒ‡æ ‡
                    db.update_key_performance(key_id, False, response_time)
                    error_detail = response.json() if response.content else {"error": {"message": "Unknown error"}}
                    if attempt == max_retries - 1:  # æœ€åä¸€æ¬¡å°è¯•
                        raise HTTPException(
                            status_code=response.status_code,
                            detail=error_detail.get("error", {}).get("message", "Unknown error")
                        )
                    else:
                        logger.warning(f"Request failed (attempt {attempt + 1}), retrying...")
                        await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                        continue

        except httpx.TimeoutException as e:
            response_time = time.time() - start_time
            db.update_key_performance(key_id, False, response_time)
            if attempt == max_retries - 1:
                raise HTTPException(status_code=504, detail="Request timeout")
            else:
                logger.warning(f"Request timeout (attempt {attempt + 1}), retrying...")
                await asyncio.sleep(2 ** attempt)
                continue
        except Exception as e:
            response_time = time.time() - start_time
            db.update_key_performance(key_id, False, response_time)
            if attempt == max_retries - 1:
                raise HTTPException(status_code=500, detail=str(e))
            else:
                logger.warning(f"Request failed (attempt {attempt + 1}): {str(e)}, retrying...")
                await asyncio.sleep(2 ** attempt)
                continue

    raise HTTPException(status_code=500, detail="Max retries exceeded")


# è¯·æ±‚å¤„ç†å‡½æ•°
async def make_request_with_failover(
        gemini_request: Dict,
        openai_request: ChatCompletionRequest,
        model_name: str,
        user_key_info: Dict = None,  # âœ… æ·»åŠ ç”¨æˆ·ä¿¡æ¯å‚æ•°
        max_key_attempts: int = None,
        excluded_keys: set = None
) -> Dict:
    """
    è¯·æ±‚å¤„ç†

    Args:
        gemini_request: è½¬æ¢åçš„Geminiè¯·æ±‚
        openai_request: åŸå§‹OpenAIè¯·æ±‚
        model_name: æ¨¡å‹åç§°
        user_key_info: ç”¨æˆ·å¯†é’¥ä¿¡æ¯
        max_key_attempts: æœ€å¤§å°è¯•keyæ•°é‡ï¼Œé»˜è®¤ä¸ºæ‰€æœ‰å¯ç”¨key
        excluded_keys: æ’é™¤çš„key IDé›†åˆ

    Returns:
        æˆåŠŸçš„Geminiå“åº”

    Raises:
        HTTPException: æ‰€æœ‰keyéƒ½å¤±è´¥æ—¶æŠ›å‡º
    """
    if excluded_keys is None:
        excluded_keys = set()

    # è·å–æ‰€æœ‰å¯ç”¨çš„key
    available_keys = db.get_available_gemini_keys()

    # è¿‡æ»¤æ’é™¤çš„key
    available_keys = [k for k in available_keys if k['id'] not in excluded_keys]

    if not available_keys:
        logger.error("No available keys for failover")
        raise HTTPException(
            status_code=503,
            detail="No available API keys"
        )

    # å¦‚æœæ²¡æœ‰æŒ‡å®šæœ€å¤§å°è¯•æ¬¡æ•°ï¼Œå°±å°è¯•æ‰€æœ‰å¯ç”¨key
    if max_key_attempts is None:
        max_key_attempts = len(available_keys)
    else:
        max_key_attempts = min(max_key_attempts, len(available_keys))

    logger.info(f"Starting failover with {max_key_attempts} key attempts for model {model_name}")

    last_error = None
    failed_keys = []

    for attempt in range(max_key_attempts):
        try:
            # é‡æ–°é€‰æ‹©å¯ç”¨çš„keyï¼ˆæ’é™¤å·²å¤±è´¥çš„ï¼‰
            selection_result = await select_gemini_key_and_check_limits(
                model_name,
                excluded_keys=excluded_keys.union(set(failed_keys))
            )

            if not selection_result:
                logger.warning(f"No more available keys after {attempt} attempts")
                break

            key_info = selection_result['key_info']
            model_config = selection_result['model_config']

            logger.info(f"Attempt {attempt + 1}: Using key #{key_info['id']} for {model_name}")

            try:
                # å°è¯•ä½¿ç”¨å½“å‰keyå‘é€è¯·æ±‚ï¼ˆåŒ…å«å•keyé‡è¯•ï¼‰
                response = await make_gemini_request_with_retry(
                    key_info['key'],
                    key_info['id'],
                    gemini_request,
                    model_name,
                    max_retries=2  # æ¯ä¸ªkeyå†…éƒ¨é‡è¯•2æ¬¡
                )

                # æˆåŠŸï¼è®°å½•ä½¿ç”¨ç»Ÿè®¡
                logger.info(f"âœ… Request successful with key #{key_info['id']} on attempt {attempt + 1}")

                # ä¼°ç®—tokenä½¿ç”¨é‡å¹¶è®°å½•
                total_tokens = 0
                for candidate in response.get("candidates", []):
                    content = candidate.get("content", {})
                    parts = content.get("parts", [])
                    for part in parts:
                        if "text" in part:
                            total_tokens += len(part["text"].split())

                # âœ… è®°å½•æˆåŠŸçš„ä½¿ç”¨ç»Ÿè®¡åˆ°æ•°æ®åº“
                if user_key_info:
                    db.log_usage(
                        gemini_key_id=key_info['id'],
                        user_key_id=user_key_info['id'],
                        model_name=model_name,
                        requests=1,
                        tokens=total_tokens
                    )
                    logger.info(
                        f"ğŸ“Š Logged usage: gemini_key_id={key_info['id']}, user_key_id={user_key_info['id']}, model={model_name}, tokens={total_tokens}")

                # è®°å½•åˆ°å†…å­˜ç¼“å­˜ï¼ˆç”¨äºé€Ÿç‡é™åˆ¶ï¼‰
                await rate_limiter.add_usage(model_name, 1, total_tokens)

                return response

            except HTTPException as e:
                # è®°å½•å¤±è´¥çš„key
                failed_keys.append(key_info['id'])
                last_error = e

                # æ›´æ–°keyæ€§èƒ½ç»Ÿè®¡ï¼ˆå¤±è´¥ï¼‰
                db.update_key_performance(key_info['id'], False, 0.0)

                # âœ… è®°å½•å¤±è´¥ç»Ÿè®¡åˆ°æ•°æ®åº“
                if user_key_info:
                    db.log_usage(
                        gemini_key_id=key_info['id'],
                        user_key_id=user_key_info['id'],
                        model_name=model_name,
                        requests=1,
                        tokens=0
                    )

                # è®°å½•å¤±è´¥ç»Ÿè®¡åˆ°å†…å­˜ç¼“å­˜
                await rate_limiter.add_usage(model_name, 1, 0)

                logger.warning(f"âŒ Key #{key_info['id']} failed with {e.status_code}: {e.detail}")

                # å¦‚æœæ˜¯å®¢æˆ·ç«¯é”™è¯¯ï¼ˆ4xxï¼‰ï¼Œå¯èƒ½æ˜¯è¯·æ±‚é—®é¢˜ï¼Œä¸å†å°è¯•å…¶ä»–key
                if e.status_code < 500:
                    logger.warning(f"Client error {e.status_code}, stopping failover")
                    raise e

                # æœåŠ¡å™¨é”™è¯¯æˆ–è¶…æ—¶ï¼Œç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªkey
                continue

        except Exception as e:
            logger.error(f"Unexpected error during failover attempt {attempt + 1}: {str(e)}")
            last_error = HTTPException(status_code=500, detail=str(e))
            continue

    # æ‰€æœ‰keyéƒ½å°è¯•å¤±è´¥äº†
    failed_count = len(failed_keys)
    logger.error(f"âŒ All {failed_count} keys failed for {model_name}")

    if last_error:
        raise last_error
    else:
        raise HTTPException(
            status_code=503,
            detail=f"All {failed_count} available API keys failed"
        )


# æµå¼å“åº”å¤„ç†å‡½æ•°
async def stream_with_failover(
        gemini_request: Dict,
        openai_request: ChatCompletionRequest,
        model_name: str,
        user_key_info: Dict = None,  # âœ… æ·»åŠ ç”¨æˆ·ä¿¡æ¯å‚æ•°
        max_key_attempts: int = None,
        excluded_keys: set = None
) -> AsyncGenerator[bytes, None]:
    """
    æµå¼å“åº”å¤„ç†
    """
    if excluded_keys is None:
        excluded_keys = set()

    available_keys = db.get_available_gemini_keys()
    available_keys = [k for k in available_keys if k['id'] not in excluded_keys]

    if not available_keys:
        error_data = {
            'error': {
                'message': 'No available API keys',
                'type': 'service_unavailable',
                'code': 503
            }
        }
        yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n".encode('utf-8')
        yield "data: [DONE]\n\n".encode('utf-8')
        return

    if max_key_attempts is None:
        max_key_attempts = len(available_keys)
    else:
        max_key_attempts = min(max_key_attempts, len(available_keys))

    logger.info(f"Starting stream failover with {max_key_attempts} key attempts for {model_name}")

    failed_keys = []

    for attempt in range(max_key_attempts):
        try:
            selection_result = await select_gemini_key_and_check_limits(
                model_name,
                excluded_keys=excluded_keys.union(set(failed_keys))
            )

            if not selection_result:
                break

            key_info = selection_result['key_info']
            logger.info(f"Stream attempt {attempt + 1}: Using key #{key_info['id']}")

            # å°è¯•æµå¼å“åº”
            success = False
            total_tokens = 0
            try:
                async for chunk in stream_gemini_response(
                        key_info['key'],
                        key_info['id'],
                        gemini_request,
                        openai_request,
                        key_info,
                        model_name
                ):
                    yield chunk
                    success = True

                # å¦‚æœæˆåŠŸå¼€å§‹æµå¼ä¼ è¾“ï¼Œè®°å½•ä½¿ç”¨ç»Ÿè®¡
                if success:
                    # âœ… è®°å½•æˆåŠŸçš„ä½¿ç”¨ç»Ÿè®¡åˆ°æ•°æ®åº“
                    if user_key_info:
                        db.log_usage(
                            gemini_key_id=key_info['id'],
                            user_key_id=user_key_info['id'],
                            model_name=model_name,
                            requests=1,
                            tokens=total_tokens  # æµå¼å“åº”ä¸­tokenè®¡ç®—æ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œæš‚æ—¶ç”¨0
                        )
                        logger.info(
                            f"ğŸ“Š Logged stream usage: gemini_key_id={key_info['id']}, user_key_id={user_key_info['id']}, model={model_name}")

                    # è®°å½•åˆ°å†…å­˜ç¼“å­˜
                    await rate_limiter.add_usage(model_name, 1, total_tokens)
                    return

            except Exception as e:
                failed_keys.append(key_info['id'])
                logger.warning(f"Stream key #{key_info['id']} failed: {str(e)}")

                # æ›´æ–°å¤±è´¥ç»Ÿè®¡
                db.update_key_performance(key_info['id'], False, 0.0)

                # âœ… è®°å½•å¤±è´¥ç»Ÿè®¡åˆ°æ•°æ®åº“
                if user_key_info:
                    db.log_usage(
                        gemini_key_id=key_info['id'],
                        user_key_id=user_key_info['id'],
                        model_name=model_name,
                        requests=1,
                        tokens=0
                    )

                # å¦‚æœè¿˜æœ‰å…¶ä»–keyå¯ä»¥å°è¯•ï¼Œç»§ç»­
                if attempt < max_key_attempts - 1:
                    # å‘é€é‡è¯•æç¤ºï¼ˆå¯é€‰ï¼‰
                    retry_msg = {
                        'error': {
                            'message': f'Key #{key_info["id"]} failed, trying next key...',
                            'type': 'retry_info',
                            'retry_attempt': attempt + 1
                        }
                    }
                    yield f"data: {json.dumps(retry_msg, ensure_ascii=False)}\n\n".encode('utf-8')
                    continue
                else:
                    # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
                    break

        except Exception as e:
            logger.error(f"Stream failover error on attempt {attempt + 1}: {str(e)}")
            continue

    # æ‰€æœ‰keyéƒ½å¤±è´¥äº†
    error_data = {
        'error': {
            'message': f'All {len(failed_keys)} available API keys failed',
            'type': 'all_keys_failed',
            'code': 503,
            'failed_keys': failed_keys
        }
    }
    yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n".encode('utf-8')
    yield "data: [DONE]\n\n".encode('utf-8')


async def stream_gemini_response(
        gemini_key: str,
        key_id: int,
        gemini_request: Dict,
        openai_request: ChatCompletionRequest,
        key_info: Dict,
        model_name: str
) -> AsyncGenerator[bytes, None]:
    """å¤„ç†Geminiçš„æµå¼å“åº”ï¼Œè®°å½•æ€§èƒ½æŒ‡æ ‡"""
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:streamGenerateContent?alt=sse"
    timeout = float(db.get_config('request_timeout', '60'))
    max_retries = int(db.get_config('max_retries', '3'))

    logger.info(f"Starting stream request to: {url}")

    start_time = time.time()

    for attempt in range(max_retries):
        try:
            async with httpx.AsyncClient(timeout=timeout) as client:
                async with client.stream(
                        "POST",
                        url,
                        json=gemini_request,
                        headers={"x-goog-api-key": gemini_key}
                ) as response:
                    if response.status_code != 200:
                        # è®°å½•å¤±è´¥
                        response_time = time.time() - start_time
                        db.update_key_performance(key_id, False, response_time)

                        error_text = await response.aread()
                        error_msg = error_text.decode() if error_text else "Unknown error"
                        logger.error(f"Stream request failed with status {response.status_code}: {error_msg}")
                        yield f"data: {json.dumps({'error': {'message': error_msg, 'type': 'api_error', 'code': response.status_code}}, ensure_ascii=False)}\n\n".encode(
                            'utf-8')
                        yield "data: [DONE]\n\n".encode('utf-8')
                        return

                    stream_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"
                    created = int(time.time())
                    total_tokens = 0
                    thinking_sent = False
                    has_content = False
                    processed_lines = 0

                    logger.info(f"Stream response started, status: {response.status_code}")

                    try:
                        # æŒ‰ç…§å®˜æ–¹æ–‡æ¡£çš„SSEæ ¼å¼å¤„ç†
                        async for line in response.aiter_lines():
                            processed_lines += 1

                            if not line:
                                continue

                            # è®°å½•åŸå§‹è¡Œä»¥ä¾¿è°ƒè¯•
                            if processed_lines <= 5:  # åªè®°å½•å‰å‡ è¡Œ
                                logger.debug(f"Stream line {processed_lines}: {line[:100]}...")

                            if line.startswith("data: "):
                                json_str = line[6:]

                                # æ£€æŸ¥ç»“æŸæ ‡å¿—
                                if json_str.strip() == "[DONE]":
                                    logger.info("Received [DONE] signal from stream")
                                    break

                                if not json_str.strip():
                                    continue

                                try:
                                    data = json.loads(json_str)

                                    # å¤„ç†å€™é€‰å“åº”
                                    for candidate in data.get("candidates", []):
                                        content_data = candidate.get("content", {})
                                        parts = content_data.get("parts", [])

                                        for part in parts:
                                            if "text" in part:
                                                text = part["text"]
                                                if not text:  # è·³è¿‡ç©ºæ–‡æœ¬
                                                    continue

                                                total_tokens += len(text.split())
                                                has_content = True

                                                # æ£€æŸ¥æ˜¯å¦ä¸ºæ€è€ƒéƒ¨åˆ†
                                                is_thought = part.get("thought", False)

                                                # å¦‚æœæ˜¯æ€è€ƒéƒ¨åˆ†ä¸”é…ç½®ä¸åŒ…å«æ€è€ƒï¼Œè·³è¿‡
                                                if is_thought and not (openai_request.thinking_config and
                                                                       openai_request.thinking_config.include_thoughts):
                                                    continue

                                                # æ€è€ƒåŠŸèƒ½çš„å¤„ç†é€»è¾‘
                                                if is_thought and not thinking_sent:
                                                    thinking_header = {
                                                        "id": stream_id,
                                                        "object": "chat.completion.chunk",
                                                        "created": created,
                                                        "model": openai_request.model,
                                                        "choices": [{
                                                            "index": 0,
                                                            "delta": {"content": "**Thinking Process:**\n"},
                                                            "finish_reason": None
                                                        }]
                                                    }
                                                    yield f"data: {json.dumps(thinking_header, ensure_ascii=False)}\n\n".encode(
                                                        'utf-8')
                                                    thinking_sent = True
                                                    logger.debug("Sent thinking header")
                                                elif not is_thought and thinking_sent:
                                                    response_header = {
                                                        "id": stream_id,
                                                        "object": "chat.completion.chunk",
                                                        "created": created,
                                                        "model": openai_request.model,
                                                        "choices": [{
                                                            "index": 0,
                                                            "delta": {"content": "\n\n**Response:**\n"},
                                                            "finish_reason": None
                                                        }]
                                                    }
                                                    yield f"data: {json.dumps(response_header, ensure_ascii=False)}\n\n".encode(
                                                        'utf-8')
                                                    thinking_sent = False
                                                    logger.debug("Sent response header")

                                                # å‘é€æ–‡æœ¬å†…å®¹
                                                chunk_data = {
                                                    "id": stream_id,
                                                    "object": "chat.completion.chunk",
                                                    "created": created,
                                                    "model": openai_request.model,
                                                    "choices": [{
                                                        "index": 0,
                                                        "delta": {"content": text},
                                                        "finish_reason": None
                                                    }]
                                                }
                                                yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n".encode(
                                                    'utf-8')

                                        # æ£€æŸ¥æ˜¯å¦ç»“æŸ
                                        finish_reason = candidate.get("finishReason")
                                        if finish_reason:
                                            finish_chunk = {
                                                "id": stream_id,
                                                "object": "chat.completion.chunk",
                                                "created": created,
                                                "model": openai_request.model,
                                                "choices": [{
                                                    "index": 0,
                                                    "delta": {},
                                                    "finish_reason": map_finish_reason(finish_reason)
                                                }]
                                            }
                                            yield f"data: {json.dumps(finish_chunk, ensure_ascii=False)}\n\n".encode(
                                                'utf-8')
                                            yield "data: [DONE]\n\n".encode('utf-8')

                                            logger.info(
                                                f"Stream completed with finish_reason: {finish_reason}, tokens: {total_tokens}")

                                            # è®°å½•æˆåŠŸçš„ä½¿ç”¨é‡å’Œæ€§èƒ½æŒ‡æ ‡
                                            response_time = time.time() - start_time
                                            db.update_key_performance(key_id, True, response_time)
                                            await rate_limiter.add_usage(model_name, 1, total_tokens)
                                            return

                                except json.JSONDecodeError as e:
                                    logger.warning(f"JSON decode error: {e}, line: {json_str[:200]}...")
                                    continue

                            # å¤„ç†å…¶ä»–SSEäº‹ä»¶ç±»å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
                            elif line.startswith("event: "):
                                # å¯ä»¥å¤„ç†ç‰¹å®šäº‹ä»¶ç±»å‹
                                continue
                            elif line.startswith("id: ") or line.startswith("retry: "):
                                # SSEå…ƒæ•°æ®ï¼Œå¿½ç•¥
                                continue

                        # å¦‚æœæµæ­£å¸¸ç»“æŸä½†æ²¡æœ‰æ˜¾å¼çš„ç»“æŸä¿¡å·
                        if has_content:
                            finish_chunk = {
                                "id": stream_id,
                                "object": "chat.completion.chunk",
                                "created": created,
                                "model": openai_request.model,
                                "choices": [{
                                    "index": 0,
                                    "delta": {},
                                    "finish_reason": "stop"
                                }]
                            }
                            yield f"data: {json.dumps(finish_chunk, ensure_ascii=False)}\n\n".encode('utf-8')
                            yield "data: [DONE]\n\n".encode('utf-8')

                            logger.info(
                                f"Stream ended naturally, processed {processed_lines} lines, tokens: {total_tokens}")

                            # è®°å½•æˆåŠŸçš„æ€§èƒ½æŒ‡æ ‡
                            response_time = time.time() - start_time
                            db.update_key_performance(key_id, True, response_time)

                        # å¦‚æœç¡®å®æ²¡æœ‰æ”¶åˆ°ä»»ä½•å†…å®¹ï¼Œæ‰å›é€€
                        if not has_content:
                            logger.warning(
                                f"Stream response had no content after processing {processed_lines} lines, falling back to non-stream")
                            try:
                                fallback_response = await make_gemini_request_with_retry(
                                    gemini_key, key_id, gemini_request, model_name, 1
                                )

                                thoughts, content = extract_thoughts_and_content(fallback_response)

                                if thoughts and openai_request.thinking_config and openai_request.thinking_config.include_thoughts:
                                    full_content = f"**Thinking Process:**\n{thoughts}\n\n**Response:**\n{content}"
                                else:
                                    full_content = content

                                if full_content:
                                    chunk_data = {
                                        "id": stream_id,
                                        "object": "chat.completion.chunk",
                                        "created": created,
                                        "model": openai_request.model,
                                        "choices": [{
                                            "index": 0,
                                            "delta": {"content": full_content},
                                            "finish_reason": None
                                        }]
                                    }
                                    yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n".encode('utf-8')

                                    finish_chunk = {
                                        "id": stream_id,
                                        "object": "chat.completion.chunk",
                                        "created": created,
                                        "model": openai_request.model,
                                        "choices": [{
                                            "index": 0,
                                            "delta": {},
                                            "finish_reason": "stop"
                                        }]
                                    }
                                    yield f"data: {json.dumps(finish_chunk, ensure_ascii=False)}\n\n".encode('utf-8')
                                    total_tokens = len(full_content.split())

                                    logger.info(f"Fallback completed, tokens: {total_tokens}")

                            except Exception as e:
                                logger.error(f"Fallback request failed: {e}")
                                response_time = time.time() - start_time
                                db.update_key_performance(key_id, False, response_time)
                                yield f"data: {json.dumps({'error': {'message': 'Failed to get response', 'type': 'server_error'}}, ensure_ascii=False)}\n\n".encode(
                                    'utf-8')

                        # è®°å½•ä½¿ç”¨é‡
                        await rate_limiter.add_usage(model_name, 1, total_tokens)
                        yield "data: [DONE]\n\n".encode('utf-8')
                        return  # æˆåŠŸå®Œæˆ

                    except (httpx.ReadError, httpx.RemoteProtocolError) as e:
                        logger.warning(f"Stream connection error (attempt {attempt + 1}): {str(e)}")
                        response_time = time.time() - start_time
                        db.update_key_performance(key_id, False, response_time)
                        if attempt < max_retries - 1:
                            yield f"data: {json.dumps({'error': {'message': 'Connection interrupted, retrying...', 'type': 'connection_error'}}, ensure_ascii=False)}\n\n".encode(
                                'utf-8')
                            await asyncio.sleep(1)
                            continue
                        else:
                            yield f"data: {json.dumps({'error': {'message': 'Stream connection failed after retries', 'type': 'connection_error'}}, ensure_ascii=False)}\n\n".encode(
                                'utf-8')
                            yield "data: [DONE]\n\n".encode('utf-8')
                            return

        except (httpx.TimeoutException, httpx.ConnectError) as e:
            logger.warning(f"Connection error (attempt {attempt + 1}): {str(e)}")
            response_time = time.time() - start_time
            db.update_key_performance(key_id, False, response_time)
            if attempt < max_retries - 1:
                yield f"data: {json.dumps({'error': {'message': f'Connection error, retrying... (attempt {attempt + 1})', 'type': 'connection_error'}}, ensure_ascii=False)}\n\n".encode(
                    'utf-8')
                await asyncio.sleep(2 ** attempt)
                continue
            else:
                yield f"data: {json.dumps({'error': {'message': 'Connection failed after all retries', 'type': 'connection_error'}}, ensure_ascii=False)}\n\n".encode(
                    'utf-8')
                yield "data: [DONE]\n\n".encode('utf-8')
                return
        except Exception as e:
            logger.error(f"Unexpected error in stream (attempt {attempt + 1}): {str(e)}")
            response_time = time.time() - start_time
            db.update_key_performance(key_id, False, response_time)
            if attempt < max_retries - 1:
                await asyncio.sleep(1)
                continue
            else:
                yield f"data: {json.dumps({'error': {'message': 'Unexpected error occurred', 'type': 'server_error'}}, ensure_ascii=False)}\n\n".encode(
                    'utf-8')
                yield "data: [DONE]\n\n".encode('utf-8')
                return


# APIç«¯ç‚¹
@app.get("/")
async def root():
    """æ ¹ç«¯ç‚¹"""
    return {
        "service": "Gemini API Proxy",
        "status": "running",
        "version": "1.1.0",
        "docs": "/docs",
        "health": "/health"
    }


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    available_keys = len(db.get_available_gemini_keys())
    uptime = time.time() - start_time

    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "available_keys": available_keys,
        "environment": "render" if os.getenv('RENDER_EXTERNAL_URL') else "local",
        "uptime_seconds": int(uptime),
        "request_count": request_count,
        "version": "1.1.0"
    }


@app.get("/wake")
async def wake_up():
    """å¿«é€Ÿå”¤é†’ç«¯ç‚¹"""
    return {
        "status": "awake",
        "timestamp": datetime.now().isoformat(),
        "message": "Service is active"
    }


@app.get("/status")
async def get_status():
    """è·å–è¯¦ç»†æœåŠ¡çŠ¶æ€"""
    import psutil
    import sys

    process = psutil.Process(os.getpid())

    return {
        "service": "Gemini API Proxy",
        "status": "running",
        "version": "1.1.0",
        "render_url": os.getenv('RENDER_EXTERNAL_URL'),
        "python_version": sys.version,
        "models": db.get_supported_models(),
        "active_keys": len(db.get_available_gemini_keys()),
        "memory_usage_mb": process.memory_info().rss / 1024 / 1024,
        "cpu_percent": process.cpu_percent(),
        "uptime_seconds": int(time.time() - start_time),
        "total_requests": request_count,
        "thinking_enabled": db.get_thinking_config()['enabled'],
        "keep_alive_active": scheduler is not None and scheduler.running
    }


@app.get("/metrics")
async def get_metrics():
    """è·å–æœåŠ¡æŒ‡æ ‡"""
    import psutil

    process = psutil.Process(os.getpid())

    return {
        "memory_usage_mb": process.memory_info().rss / 1024 / 1024,
        "cpu_percent": process.cpu_percent(),
        "active_connections": len(db.get_available_gemini_keys()),
        "uptime_seconds": int(time.time() - start_time),
        "requests_count": request_count,
        "database_size_mb": os.path.getsize(db.db_path) / 1024 / 1024 if os.path.exists(db.db_path) else 0
    }


@app.get("/v1")
async def api_v1_info():
    """v1 API ä¿¡æ¯ç«¯ç‚¹ - æä¾› API ç‰ˆæœ¬ä¿¡æ¯å’Œå¯ç”¨ç«¯ç‚¹"""
    available_keys = len(db.get_available_gemini_keys())
    supported_models = db.get_supported_models()
    thinking_config = db.get_thinking_config()

    # è·å–å½“å‰æœåŠ¡çš„åŸºç¡€URL
    render_url = os.getenv('RENDER_EXTERNAL_URL')
    base_url = render_url if render_url else 'https://your-service.onrender.com'

    return {
        "service": "Gemini API Proxy",
        "version": "1.1.0",
        "api_version": "v1",
        "compatibility": "OpenAI API v1",
        "description": "A high-performance proxy for Gemini API with OpenAI compatibility and multi-key polling",
        "status": "operational",
        "base_url": base_url,
        "features": [
            "Multi-key polling & load balancing",
            "OpenAI API compatibility",
            "Rate limiting & usage analytics",
            "Thinking mode support",
            "Streaming responses",
            "Automatic failover",
            "Real-time monitoring",
            "Health checking",
            "Adaptive load balancing"
        ],
        "endpoints": {
            "chat_completions": "/v1/chat/completions",
            "models": "/v1/models",
            "api_info": "/v1",
            "health": "/health",
            "status": "/status",
            "admin": "/admin/*",
            "docs": "/docs"
        },
        "supported_models": supported_models,
        "service_status": {
            "active_gemini_keys": available_keys,
            "thinking_enabled": thinking_config.get('enabled', False),
            "thinking_budget": thinking_config.get('budget', -1),
            "uptime_seconds": int(time.time() - start_time),
            "total_requests": request_count
        },
        "documentation": {
            "openapi_json": "/openapi.json",
            "swagger_ui": "/docs",
            "redoc": "/redoc",
            "github": "https://github.com/arain119/gemini-api-proxy"
        },
        "example_usage": {
            "curl": f"curl -X POST '{base_url}/v1/chat/completions' \\\n  -H 'Authorization: Bearer YOUR_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{{\n    \"model\": \"gemini-2.5-flash\",\n    \"messages\": [{{\"role\": \"user\", \"content\": \"Hello!\"}}]\n  }}'",
            "python": f"import openai\n\nclient = openai.OpenAI(\n    api_key='YOUR_API_KEY',\n    base_url='{base_url}/v1'\n)\n\nresponse = client.chat.completions.create(\n    model='gemini-2.5-flash',\n    messages=[{{'role': 'user', 'content': 'Hello!'}}]\n)",
            "javascript": f"import OpenAI from 'openai';\n\nconst openai = new OpenAI({{\n  apiKey: 'YOUR_API_KEY',\n  baseURL: '{base_url}/v1'\n}});\n\nconst response = await openai.chat.completions.create({{\n  model: 'gemini-2.5-flash',\n  messages: [{{ role: 'user', content: 'Hello!' }}]\n}});"
        },
        "rate_limits": {
            "info": "Limits scale with number of healthy Gemini API keys",
            "check_admin": "/admin/models for current limits"
        },
        "timestamp": datetime.now().isoformat()
    }


# chat_completionsç«¯ç‚¹
@app.post("/v1/chat/completions")
async def chat_completions(
        request: ChatCompletionRequest,
        authorization: str = Header(None)
):
    try:
        # éªŒè¯API Key
        if not authorization or not authorization.startswith("Bearer "):
            raise HTTPException(status_code=401, detail="Invalid authorization header")

        api_key = authorization.replace("Bearer ", "")
        user_key = db.validate_user_key(api_key)

        if not user_key:
            raise HTTPException(status_code=401, detail="Invalid API key")

        # âœ… è·å–ç”¨æˆ·å¯†é’¥ä¿¡æ¯ï¼Œç”¨äºè®°å½•ä½¿ç”¨ç»Ÿè®¡
        user_key_info = user_key

        # åŸºç¡€è¯·æ±‚éªŒè¯
        if not request.messages or len(request.messages) == 0:
            raise HTTPException(status_code=422, detail="Messages cannot be empty")

        # éªŒè¯æ¶ˆæ¯æ ¼å¼
        for msg in request.messages:
            if not hasattr(msg, 'role') or not hasattr(msg, 'content'):
                raise HTTPException(status_code=422, detail="Invalid message format")
            if msg.role not in ['system', 'user', 'assistant']:
                raise HTTPException(status_code=422, detail=f"Invalid role: {msg.role}")

            # ç¡®ä¿contentå·²ç»è¢«æ ‡å‡†åŒ–ä¸ºå­—ç¬¦ä¸²
            if not isinstance(msg.content, str):
                raise HTTPException(status_code=422, detail="Content validation failed")

        # è·å–å®é™…ä½¿ç”¨çš„æ¨¡å‹åç§°
        actual_model_name = get_actual_model_name(request.model)

        # æ³¨å…¥promptï¼ˆå¦‚æœå¯ç”¨ï¼‰
        request.messages = inject_prompt_to_messages(request.messages)

        # è½¬æ¢è¯·æ±‚æ ¼å¼
        gemini_request = openai_to_gemini(request)

        # æ•…éšœè½¬ç§»æœºåˆ¶
        if request.stream:
            # âœ… æµå¼å“åº”ä½¿ç”¨æ•…éšœè½¬ç§»ï¼Œä¼ é€’ç”¨æˆ·ä¿¡æ¯
            return StreamingResponse(
                stream_with_failover(
                    gemini_request,
                    request,
                    actual_model_name,
                    user_key_info=user_key_info,  # ä¼ é€’ç”¨æˆ·ä¿¡æ¯
                    max_key_attempts=5  # æœ€å¤šå°è¯•5ä¸ªkey
                ),
                media_type="text/event-stream; charset=utf-8"
            )
        else:
            # âœ… éæµå¼å“åº”ä½¿ç”¨æ•…éšœè½¬ç§»ï¼Œä¼ é€’ç”¨æˆ·ä¿¡æ¯
            gemini_response = await make_request_with_failover(
                gemini_request,
                request,
                actual_model_name,
                user_key_info=user_key_info,  # ä¼ é€’ç”¨æˆ·ä¿¡æ¯
                max_key_attempts=5  # æœ€å¤šå°è¯•5ä¸ªkey
            )

            # è®¡ç®—tokenä½¿ç”¨é‡
            total_tokens = 0
            for candidate in gemini_response.get("candidates", []):
                content = candidate.get("content", {})
                parts = content.get("parts", [])
                for part in parts:
                    if "text" in part:
                        total_tokens += len(part["text"].split())

            # è½¬æ¢å“åº”æ ¼å¼
            usage_info = {
                "prompt_tokens": len(str(request.messages).split()),
                "completion_tokens": total_tokens,
                "total_tokens": len(str(request.messages).split()) + total_tokens
            }

            openai_response = gemini_to_openai(gemini_response, request, usage_info)
            return JSONResponse(content=openai_response)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/v1/models")
async def list_models():
    """åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹"""
    models = db.get_supported_models()

    model_list = []
    for model in models:
        model_list.append({
            "id": model,
            "object": "model",
            "created": int(time.time()),
            "owned_by": "google"
        })

    return {"object": "list", "data": model_list}


# æ–‡ä»¶ä¸Šä¼ ç›¸å…³ç«¯ç‚¹
@app.post("/v1/files")
async def upload_file(
        file: UploadFile = File(...),
        authorization: str = Header(None)
):
    """ä¸Šä¼ æ–‡ä»¶ç”¨äºå¤šæ¨¡æ€å¯¹è¯"""
    try:
        # éªŒè¯API Key
        if not authorization or not authorization.startswith("Bearer "):
            raise HTTPException(status_code=401, detail="Invalid authorization header")

        api_key = authorization.replace("Bearer ", "")
        user_key = db.validate_user_key(api_key)

        if not user_key:
            raise HTTPException(status_code=401, detail="Invalid API key")

        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_content = await file.read()
        if len(file_content) > MAX_FILE_SIZE:
            raise HTTPException(
                status_code=413,
                detail=f"File too large. Maximum size is {MAX_FILE_SIZE // (1024 * 1024)}MB"
            )

        # æ£€æŸ¥MIMEç±»å‹
        mime_type = file.content_type or mimetypes.guess_type(file.filename)[0]
        if mime_type not in SUPPORTED_MIME_TYPES:
            raise HTTPException(
                status_code=400,
                detail=f"Unsupported file type: {mime_type}. Supported types: {', '.join(SUPPORTED_MIME_TYPES)}"
            )

        # ç”Ÿæˆæ–‡ä»¶ID
        file_id = f"file-{uuid.uuid4().hex}"

        # åˆ¤æ–­æ˜¯å¦ä¸ºå°æ–‡ä»¶ï¼ˆ20MBä»¥ä¸‹ä½¿ç”¨å†…è”æ•°æ®ï¼‰
        is_small_file = len(file_content) <= 20 * 1024 * 1024

        file_info = {
            "id": file_id,
            "object": "file",
            "bytes": len(file_content),
            "created_at": int(time.time()),
            "filename": file.filename,
            "purpose": "multimodal",
            "mime_type": mime_type,
            "is_small_file": is_small_file
        }

        if is_small_file:
            # å°æ–‡ä»¶ï¼šå­˜å‚¨base64ç¼–ç çš„æ•°æ®
            file_info["data"] = base64.b64encode(file_content).decode('utf-8')
        else:
            # å¤§æ–‡ä»¶ï¼šä¿å­˜åˆ°ç£ç›˜å¹¶å­˜å‚¨æ–‡ä»¶è·¯å¾„
            file_path = os.path.join(UPLOAD_DIR, f"{file_id}_{file.filename}")
            with open(file_path, "wb") as f:
                f.write(file_content)
            file_info["file_path"] = file_path
            file_info["file_uri"] = f"file://{os.path.abspath(file_path)}"

        # å­˜å‚¨æ–‡ä»¶ä¿¡æ¯
        file_storage[file_id] = file_info

        logger.info(f"File uploaded: {file_id}, size: {len(file_content)} bytes, type: {mime_type}")

        return {
            "id": file_id,
            "object": "file",
            "bytes": len(file_content),
            "created_at": file_info["created_at"],
            "filename": file.filename,
            "purpose": "multimodal"
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"File upload failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/v1/files")
async def list_files(authorization: str = Header(None)):
    """åˆ—å‡ºå·²ä¸Šä¼ çš„æ–‡ä»¶"""
    try:
        # éªŒè¯API Key
        if not authorization or not authorization.startswith("Bearer "):
            raise HTTPException(status_code=401, detail="Invalid authorization header")

        api_key = authorization.replace("Bearer ", "")
        user_key = db.validate_user_key(api_key)

        if not user_key:
            raise HTTPException(status_code=401, detail="Invalid API key")

        files = []
        for file_id, file_info in file_storage.items():
            files.append({
                "id": file_id,
                "object": "file",
                "bytes": file_info["bytes"],
                "created_at": file_info["created_at"],
                "filename": file_info["filename"],
                "purpose": file_info["purpose"]
            })

        return {
            "object": "list",
            "data": files
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"List files failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/v1/files/{file_id}")
async def get_file(file_id: str, authorization: str = Header(None)):
    """è·å–æ–‡ä»¶ä¿¡æ¯"""
    try:
        # éªŒè¯API Key
        if not authorization or not authorization.startswith("Bearer "):
            raise HTTPException(status_code=401, detail="Invalid authorization header")

        api_key = authorization.replace("Bearer ", "")
        user_key = db.validate_user_key(api_key)

        if not user_key:
            raise HTTPException(status_code=401, detail="Invalid API key")

        if file_id not in file_storage:
            raise HTTPException(status_code=404, detail="File not found")

        file_info = file_storage[file_id]
        return {
            "id": file_id,
            "object": "file",
            "bytes": file_info["bytes"],
            "created_at": file_info["created_at"],
            "filename": file_info["filename"],
            "purpose": file_info["purpose"]
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Get file failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/v1/files/{file_id}")
async def delete_file(file_id: str, authorization: str = Header(None)):
    """åˆ é™¤æ–‡ä»¶"""
    try:
        # éªŒè¯API Key
        if not authorization or not authorization.startswith("Bearer "):
            raise HTTPException(status_code=401, detail="Invalid authorization header")

        api_key = authorization.replace("Bearer ", "")
        user_key = db.validate_user_key(api_key)

        if not user_key:
            raise HTTPException(status_code=401, detail="Invalid API key")

        if file_id not in file_storage:
            raise HTTPException(status_code=404, detail="File not found")

        file_info = file_storage[file_id]

        # å¦‚æœæ˜¯å¤§æ–‡ä»¶ï¼Œåˆ é™¤ç£ç›˜æ–‡ä»¶
        if "file_path" in file_info and os.path.exists(file_info["file_path"]):
            os.remove(file_info["file_path"])

        # ä»å­˜å‚¨ä¸­åˆ é™¤
        del file_storage[file_id]

        logger.info(f"File deleted: {file_id}")

        return {
            "id": file_id,
            "object": "file",
            "deleted": True
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Delete file failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# å¥åº·æ£€æµ‹ç›¸å…³ç«¯ç‚¹
@app.post("/admin/health/check-all")
async def check_all_keys_health():
    """ä¸€é”®æ£€æµ‹æ‰€æœ‰Gemini Keysçš„å¥åº·çŠ¶æ€"""
    try:
        all_keys = db.get_all_gemini_keys()
        active_keys = [key for key in all_keys if key['status'] == 1]

        if not active_keys:
            return {
                "success": True,
                "message": "No active keys to check",
                "results": []
            }

        results = []
        healthy_count = 0

        # å¹¶å‘æ£€æµ‹æ‰€æœ‰keys
        tasks = []
        for key_info in active_keys:
            task = check_gemini_key_health(key_info['key'])
            tasks.append((key_info['id'], task))

        # ç­‰å¾…æ‰€æœ‰æ£€æµ‹å®Œæˆ
        for key_id, task in tasks:
            health_result = await task

            # æ›´æ–°æ•°æ®åº“ä¸­çš„å¥åº·çŠ¶æ€
            db.update_key_performance(
                key_id,
                health_result['healthy'],
                health_result['response_time']
            )

            if health_result['healthy']:
                healthy_count += 1

            results.append({
                "key_id": key_id,
                "healthy": health_result['healthy'],
                "response_time": health_result['response_time'],
                "error": health_result['error']
            })

        return {
            "success": True,
            "message": f"Health check completed: {healthy_count}/{len(active_keys)} keys healthy",
            "total_checked": len(active_keys),
            "healthy_count": healthy_count,
            "unhealthy_count": len(active_keys) - healthy_count,
            "results": results
        }

    except Exception as e:
        logger.error(f"Health check failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/admin/health/summary")
async def get_health_summary():
    """è·å–å¥åº·çŠ¶æ€æ±‡æ€»"""
    try:
        summary = db.get_keys_health_summary()
        return {
            "success": True,
            "summary": summary
        }
    except Exception as e:
        logger.error(f"Failed to get health summary: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# å¯†é’¥ç®¡ç†ç«¯ç‚¹
@app.get("/admin/keys/gemini")
async def get_gemini_keys():
    """è·å–æ‰€æœ‰Geminiå¯†é’¥åˆ—è¡¨"""
    try:
        keys = db.get_all_gemini_keys()
        return {
            "success": True,
            "keys": keys
        }
    except Exception as e:
        logger.error(f"Failed to get Gemini keys: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/admin/keys/user")
async def get_user_keys():
    """è·å–æ‰€æœ‰ç”¨æˆ·å¯†é’¥åˆ—è¡¨"""
    try:
        keys = db.get_all_user_keys()
        return {
            "success": True,
            "keys": keys
        }
    except Exception as e:
        logger.error(f"Failed to get user keys: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/admin/keys/gemini/{key_id}")
async def delete_gemini_key(key_id: int):
    """åˆ é™¤æŒ‡å®šçš„Geminiå¯†é’¥"""
    try:
        success = db.delete_gemini_key(key_id)
        if success:
            logger.info(f"Deleted Gemini key #{key_id}")
            return {
                "success": True,
                "message": f"Gemini key #{key_id} deleted successfully"
            }
        else:
            raise HTTPException(status_code=404, detail="Key not found")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to delete Gemini key #{key_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/admin/keys/user/{key_id}")
async def delete_user_key(key_id: int):
    """åˆ é™¤æŒ‡å®šçš„ç”¨æˆ·å¯†é’¥"""
    try:
        success = db.delete_user_key(key_id)
        if success:
            logger.info(f"Deleted user key #{key_id}")
            return {
                "success": True,
                "message": f"User key #{key_id} deleted successfully"
            }
        else:
            raise HTTPException(status_code=404, detail="Key not found")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to delete user key #{key_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/admin/keys/gemini/{key_id}/toggle")
async def toggle_gemini_key_status(key_id: int):
    """åˆ‡æ¢Geminiå¯†é’¥çŠ¶æ€"""
    try:
        success = db.toggle_gemini_key_status(key_id)
        if success:
            logger.info(f"Toggled Gemini key #{key_id} status")
            return {
                "success": True,
                "message": f"Gemini key #{key_id} status toggled successfully"
            }
        else:
            raise HTTPException(status_code=404, detail="Key not found")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to toggle Gemini key #{key_id} status: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/admin/keys/user/{key_id}/toggle")
async def toggle_user_key_status(key_id: int):
    """åˆ‡æ¢ç”¨æˆ·å¯†é’¥çŠ¶æ€"""
    try:
        success = db.toggle_user_key_status(key_id)
        if success:
            logger.info(f"Toggled user key #{key_id} status")
            return {
                "success": True,
                "message": f"User key #{key_id} status toggled successfully"
            }
        else:
            raise HTTPException(status_code=404, detail="Key not found")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to toggle user key #{key_id} status: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# ç®¡ç†ç«¯ç‚¹
@app.get("/admin/models/{model_name}")
async def get_model_config(model_name: str):
    """è·å–æŒ‡å®šæ¨¡å‹çš„é…ç½®"""
    try:
        model_config = db.get_model_config(model_name)
        if not model_config:
            raise HTTPException(status_code=404, detail=f"Model {model_name} not found")

        return {
            "success": True,
            "model_name": model_name,
            **model_config
        }
    except Exception as e:
        logger.error(f"Failed to get model config for {model_name}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/admin/models/{model_name}")
async def update_model_config(model_name: str, request: dict):
    """æ›´æ–°æŒ‡å®šæ¨¡å‹çš„é…ç½®"""
    try:
        # éªŒè¯æ¨¡å‹æ˜¯å¦å­˜åœ¨
        if model_name not in db.get_supported_models():
            raise HTTPException(status_code=404, detail=f"Model {model_name} not supported")

        # æå–å…è®¸æ›´æ–°çš„å­—æ®µ
        allowed_fields = ['single_api_rpm_limit', 'single_api_tpm_limit', 'single_api_rpd_limit', 'status']
        update_data = {}

        for field in allowed_fields:
            if field in request:
                update_data[field] = request[field]

        if not update_data:
            raise HTTPException(status_code=422, detail="No valid fields to update")

        # æ›´æ–°æ¨¡å‹é…ç½®
        success = db.update_model_config(model_name, **update_data)

        if success:
            logger.info(f"Updated model config for {model_name}: {update_data}")
            return {
                "success": True,
                "message": f"Model {model_name} configuration updated successfully",
                "updated_fields": update_data
            }
        else:
            raise HTTPException(status_code=500, detail="Failed to update model configuration")

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to update model config for {model_name}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/admin/models")
async def list_model_configs():
    """è·å–æ‰€æœ‰æ¨¡å‹çš„é…ç½®"""
    try:
        model_configs = db.get_all_model_configs()
        return {
            "success": True,
            "models": model_configs
        }
    except Exception as e:
        logger.error(f"Failed to get model configs: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/admin/config/gemini-key")
async def add_gemini_key(request: dict):
    """é€šè¿‡APIæ·»åŠ Geminiå¯†é’¥ï¼Œæ”¯æŒæ‰¹é‡æ·»åŠ """
    input_keys = request.get("key", "").strip()

    if not input_keys:
        return {"success": False, "message": "è¯·æä¾›APIå¯†é’¥"}

    # æ£€æµ‹æ˜¯å¦åŒ…å«åˆ†éš”ç¬¦ï¼Œæ”¯æŒæ‰¹é‡æ·»åŠ 
    separators = [',', ';', '\n', '\r\n', '\r', '\t']  # é€—å·ã€åˆ†å·ã€æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦
    has_separator = any(sep in input_keys for sep in separators)

    # å¦‚æœåŒ…å«åˆ†éš”ç¬¦æˆ–å¤šä¸ªç©ºæ ¼ï¼Œè¿›è¡Œåˆ†å‰²
    if has_separator or '  ' in input_keys:  # ä¸¤ä¸ªæˆ–æ›´å¤šç©ºæ ¼ä¹Ÿè§†ä¸ºåˆ†éš”ç¬¦
        # é¦–å…ˆæŒ‰æ¢è¡Œç¬¦åˆ†å‰²
        lines = input_keys.replace('\r\n', '\n').replace('\r', '\n').split('\n')

        keys_to_add = []
        for line in lines:
            # å†æŒ‰å…¶ä»–åˆ†éš”ç¬¦åˆ†å‰²æ¯ä¸€è¡Œ
            line_keys = []
            for sep in [',', ';', '\t']:
                if sep in line:
                    line_keys.extend([k.strip() for k in line.split(sep)])
                    break
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ†éš”ç¬¦ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªç©ºæ ¼
                if '  ' in line:  # å¤šä¸ªç©ºæ ¼
                    line_keys.extend([k.strip() for k in line.split()])
                else:
                    line_keys.append(line.strip())

            keys_to_add.extend(line_keys)

        # æ¸…ç†ç©ºå­—ç¬¦ä¸²
        keys_to_add = [key for key in keys_to_add if key]

        logger.info(f"æ£€æµ‹åˆ°æ‰¹é‡æ·»åŠ æ¨¡å¼ï¼Œå°†æ·»åŠ  {len(keys_to_add)} ä¸ªå¯†é’¥")

    else:
        # å•ä¸ªå¯†é’¥
        keys_to_add = [input_keys]

    # éªŒè¯å’Œæ·»åŠ å¯†é’¥
    results = {
        "success": True,
        "total_processed": len(keys_to_add),
        "successful_adds": 0,
        "failed_adds": 0,
        "details": [],
        "invalid_keys": [],
        "duplicate_keys": []
    }

    for i, key in enumerate(keys_to_add, 1):
        key = key.strip()

        # éªŒè¯å¯†é’¥æ ¼å¼
        if not key:
            continue

        if not key.startswith('AIzaSy'):
            results["invalid_keys"].append(f"#{i}: {key[:20]}... (ä¸æ˜¯æœ‰æ•ˆçš„Gemini APIå¯†é’¥æ ¼å¼)")
            results["failed_adds"] += 1
            continue

        if len(key) < 30 or len(key) > 50:  # Gemini API Key é•¿åº¦é€šå¸¸åœ¨35-40å­—ç¬¦
            results["invalid_keys"].append(f"#{i}: {key[:20]}... (å¯†é’¥é•¿åº¦å¼‚å¸¸)")
            results["failed_adds"] += 1
            continue

        # å°è¯•æ·»åŠ åˆ°æ•°æ®åº“
        try:
            if db.add_gemini_key(key):
                results["successful_adds"] += 1
                results["details"].append(f"âœ… #{i}: {key[:10]}...{key[-4:]} æ·»åŠ æˆåŠŸ")
                logger.info(f"æˆåŠŸæ·»åŠ Geminiå¯†é’¥ #{i}")
            else:
                results["duplicate_keys"].append(f"#{i}: {key[:10]}...{key[-4:]} (å¯†é’¥å·²å­˜åœ¨)")
                results["failed_adds"] += 1
        except Exception as e:
            results["failed_adds"] += 1
            results["details"].append(f"âŒ #{i}: {key[:10]}...{key[-4:]} æ·»åŠ å¤±è´¥ - {str(e)}")
            logger.error(f"æ·»åŠ Geminiå¯†é’¥ #{i} å¤±è´¥: {str(e)}")

    # ç”Ÿæˆè¿”å›æ¶ˆæ¯
    if results["successful_adds"] > 0:
        message_parts = [f"æˆåŠŸæ·»åŠ  {results['successful_adds']} ä¸ªå¯†é’¥"]

        if results["failed_adds"] > 0:
            message_parts.append(f"å¤±è´¥ {results['failed_adds']} ä¸ª")

        results["message"] = "ã€".join(message_parts)

        # å¦‚æœæœ‰éƒ¨åˆ†æˆåŠŸï¼Œæ•´ä½“ä»è§†ä¸ºæˆåŠŸ
        results["success"] = True
    else:
        results["success"] = False
        results["message"] = f"æ‰€æœ‰ {results['total_processed']} ä¸ªå¯†é’¥æ·»åŠ å¤±è´¥"

    # è¯¦ç»†æ—¥å¿—
    logger.info(
        f"æ‰¹é‡æ·»åŠ ç»“æœ: å¤„ç†{results['total_processed']}ä¸ªï¼ŒæˆåŠŸ{results['successful_adds']}ä¸ªï¼Œå¤±è´¥{results['failed_adds']}ä¸ª")

    return results


@app.post("/admin/config/user-key")
async def generate_user_key(request: dict):
    """ç”Ÿæˆç”¨æˆ·å¯†é’¥"""
    name = request.get("name", "API User")
    key = db.generate_user_key(name)
    logger.info(f"Generated new user key for: {name}")
    return {"success": True, "key": key, "name": name}


@app.post("/admin/config/thinking")
async def update_thinking_config(request: dict):
    """æ›´æ–°æ€è€ƒæ¨¡å¼é…ç½®"""
    try:
        enabled = request.get('enabled')
        budget = request.get('budget')
        include_thoughts = request.get('include_thoughts')

        success = db.set_thinking_config(
            enabled=enabled,
            budget=budget,
            include_thoughts=include_thoughts
        )

        if success:
            logger.info(f"Updated thinking config: {request}")
            return {
                "success": True,
                "message": "Thinking configuration updated successfully"
            }
        else:
            raise HTTPException(status_code=500, detail="Failed to update thinking configuration")

    except ValueError as e:
        raise HTTPException(status_code=422, detail=str(e))
    except Exception as e:
        logger.error(f"Failed to update thinking config: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/admin/config/inject-prompt")
async def update_inject_prompt_config(request: dict):
    """æ›´æ–°æç¤ºè¯æ³¨å…¥é…ç½®"""
    try:
        enabled = request.get('enabled')
        content = request.get('content')
        position = request.get('position')

        success = db.set_inject_prompt_config(
            enabled=enabled,
            content=content,
            position=position
        )

        if success:
            logger.info(f"Updated inject prompt config: enabled={enabled}, position={position}")
            return {
                "success": True,
                "message": "Inject prompt configuration updated successfully"
            }
        else:
            raise HTTPException(status_code=500, detail="Failed to update inject prompt configuration")

    except ValueError as e:
        raise HTTPException(status_code=422, detail=str(e))
    except Exception as e:
        logger.error(f"Failed to update inject prompt config: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/admin/config")
async def get_all_config():
    """è·å–æ‰€æœ‰ç³»ç»Ÿé…ç½®"""
    try:
        configs = db.get_all_configs()
        thinking_config = db.get_thinking_config()
        inject_config = db.get_inject_prompt_config()

        return {
            "success": True,
            "system_configs": configs,
            "thinking_config": thinking_config,
            "inject_config": inject_config
        }
    except Exception as e:
        logger.error(f"Failed to get configs: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/admin/stats")
async def get_admin_stats():
    """è·å–ç®¡ç†ç»Ÿè®¡"""
    health_summary = db.get_keys_health_summary()

    return {
        "gemini_keys": len(db.get_all_gemini_keys()),
        "active_gemini_keys": len(db.get_available_gemini_keys()),
        "healthy_gemini_keys": health_summary['healthy'],
        "user_keys": len(db.get_all_user_keys()),
        "active_user_keys": len([k for k in db.get_all_user_keys() if k['status'] == 1]),
        "supported_models": db.get_supported_models(),
        "usage_stats": db.get_all_usage_stats(),
        "thinking_config": db.get_thinking_config(),
        "inject_config": db.get_inject_prompt_config(),
        "health_summary": health_summary
    }


# è¿è¡ŒæœåŠ¡å™¨çš„å‡½æ•°
def run_api_server(port: int = 8000):
    """è¿è¡ŒAPIæœåŠ¡å™¨"""
    uvicorn.run(app, host="0.0.0.0", port=port, log_level="info")


if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    logger.info(f"Starting Gemini API Proxy on port {port}")
    run_api_server(port)