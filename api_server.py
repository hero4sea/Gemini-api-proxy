import asyncio
import json
import time
import uuid
import logging
import os
import sys
from datetime import datetime
from typing import Dict, List, Optional, AsyncGenerator, Union, Any
from contextlib import asynccontextmanager

import httpx
import uvicorn
from fastapi import FastAPI, HTTPException, Request, Header
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.exceptions import RequestValidationError
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, ValidationError, validator
from apscheduler.schedulers.asyncio import AsyncIOScheduler

from database import Database

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡
start_time = time.time()
request_count = 0


# æ€è€ƒé…ç½®æ¨¡å‹
class ThinkingConfig(BaseModel):
    thinking_budget: Optional[int] = None  # 0-32768, 0=ç¦ç”¨æ€è€ƒ, None=è‡ªåŠ¨
    include_thoughts: Optional[bool] = False  # æ˜¯å¦åœ¨å“åº”ä¸­åŒ…å«æ€è€ƒè¿‡ç¨‹

    class Config:
        extra = "allow"

    @validator('thinking_budget')
    def validate_thinking_budget(cls, v):
        if v is not None:
            if not isinstance(v, int) or v < 0 or v > 32768:
                raise ValueError("thinking_budget must be an integer between 0 and 32768")
        return v


# è¯·æ±‚/å“åº”æ¨¡å‹
class ChatMessage(BaseModel):
    role: str
    content: Union[str, List[Dict[str, Any]]]  # æ”¯æŒå­—ç¬¦ä¸²å’Œæ•°ç»„æ ¼å¼

    class Config:
        # å…è®¸é¢å¤–å­—æ®µï¼Œæé«˜å…¼å®¹æ€§
        extra = "allow"

    @validator('content')
    def validate_content(cls, v):
        """éªŒè¯å¹¶æ ‡å‡†åŒ–contentå­—æ®µ"""
        if isinstance(v, str):
            return v
        elif isinstance(v, list):
            # å°†æ•°ç»„æ ¼å¼è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            text_parts = []
            for item in v:
                if isinstance(item, dict):
                    if item.get('type') == 'text' and 'text' in item:
                        text_parts.append(item['text'])
                    elif 'text' in item:  # å…¼å®¹å…¶ä»–å¯èƒ½çš„æ ¼å¼
                        text_parts.append(item['text'])
                elif isinstance(item, str):
                    text_parts.append(item)
            return ' '.join(text_parts) if text_parts else ""
        else:
            raise ValueError("content must be string or array of content objects")

    def get_text_content(self) -> str:
        """è·å–çº¯æ–‡æœ¬å†…å®¹"""
        # ç”±äºvalidatorå·²ç»å°†contentæ ‡å‡†åŒ–ä¸ºå­—ç¬¦ä¸²ï¼Œè¿™é‡Œç›´æ¥è¿”å›
        return self.content if isinstance(self.content, str) else str(self.content)


class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    temperature: Optional[float] = 1.0
    top_p: Optional[float] = 1.0
    n: Optional[int] = 1
    stream: Optional[bool] = False
    stop: Optional[List[str]] = None
    max_tokens: Optional[int] = None
    presence_penalty: Optional[float] = 0
    frequency_penalty: Optional[float] = 0
    user: Optional[str] = None
    # æ€è€ƒé…ç½®
    thinking_config: Optional[ThinkingConfig] = None

    class Config:
        # å…è®¸é¢å¤–å­—æ®µï¼Œæé«˜ä¸OpenAI SDKçš„å…¼å®¹æ€§
        extra = "allow"

    # è‡ªå®šä¹‰éªŒè¯å™¨ï¼Œç¡®ä¿å‚æ•°åœ¨åˆç†èŒƒå›´å†…
    def __init__(self, **data):
        # å‚æ•°èŒƒå›´éªŒè¯
        if 'temperature' in data and data['temperature'] is not None:
            data['temperature'] = max(0.0, min(2.0, data['temperature']))
        if 'top_p' in data and data['top_p'] is not None:
            data['top_p'] = max(0.0, min(1.0, data['top_p']))
        if 'n' in data and data['n'] is not None:
            data['n'] = max(1, min(4, data['n']))
        if 'max_tokens' in data and data['max_tokens'] is not None:
            data['max_tokens'] = max(1, min(8192, data['max_tokens']))

        super().__init__(**data)


# å†…å­˜ç¼“å­˜ç”¨äºRPM/TPMé™åˆ¶
class RateLimitCache:
    def __init__(self):
        self.cache: Dict[str, Dict[str, List[tuple]]] = {}  # æ”¹ä¸ºæŒ‰æ¨¡å‹ç¼“å­˜
        self.lock = asyncio.Lock()

    async def add_usage(self, model_name: str, requests: int = 1, tokens: int = 0):
        async with self.lock:
            if model_name not in self.cache:
                self.cache[model_name] = {'requests': [], 'tokens': []}

            current_time = time.time()
            self.cache[model_name]['requests'].append((current_time, requests))
            self.cache[model_name]['tokens'].append((current_time, tokens))

    async def get_current_usage(self, model_name: str, window_seconds: int = 60) -> Dict[str, int]:
        async with self.lock:
            if model_name not in self.cache:
                return {'requests': 0, 'tokens': 0}

            current_time = time.time()
            cutoff_time = current_time - window_seconds

            # æ¸…ç†è¿‡æœŸè®°å½•
            self.cache[model_name]['requests'] = [
                (t, v) for t, v in self.cache[model_name]['requests']
                if t > cutoff_time
            ]
            self.cache[model_name]['tokens'] = [
                (t, v) for t, v in self.cache[model_name]['tokens']
                if t > cutoff_time
            ]

            # è®¡ç®—æ€»å’Œ
            total_requests = sum(v for _, v in self.cache[model_name]['requests'])
            total_tokens = sum(v for _, v in self.cache[model_name]['tokens'])

            return {'requests': total_requests, 'tokens': total_tokens}


# ä¿æŒå”¤é†’æœºåˆ¶ï¼ˆä»…åœ¨Renderç¯å¢ƒå¯ç”¨ï¼‰
async def keep_alive():
    """ä¿æŒæœåŠ¡å”¤é†’"""
    try:
        render_url = os.getenv('RENDER_EXTERNAL_URL')
        if render_url:
            async with httpx.AsyncClient(timeout=10) as client:
                await client.get(f"{render_url}/wake")
                logger.info("Keep-alive ping sent successfully")
    except Exception as e:
        logger.warning(f"Keep-alive ping failed: {e}")


# å…¨å±€å˜é‡
db = Database()
rate_limiter = RateLimitCache()
scheduler = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    global scheduler
    # å¯åŠ¨æ—¶çš„æ“ä½œ
    logger.info("ğŸš€ Gemini API Proxy starting...")
    logger.info(f"ğŸ“Š Available API keys: {len(db.get_available_gemini_keys())}")
    logger.info(f"ğŸŒ Environment: {'Render' if os.getenv('RENDER_EXTERNAL_URL') else 'Local'}")

    # å¯åŠ¨ä¿æŒå”¤é†’è°ƒåº¦å™¨ï¼ˆä»…åœ¨Renderç¯å¢ƒï¼‰
    render_url = os.getenv('RENDER_EXTERNAL_URL')
    if render_url:
        scheduler = AsyncIOScheduler()
        scheduler.add_job(
            keep_alive,
            'interval',
            minutes=14,  # åœ¨15åˆ†é’Ÿç¡çœ å‰ä¿æŒå”¤é†’
            id='keep_alive',
            max_instances=1
        )
        scheduler.start()
        logger.info("â° Keep-alive scheduler started (14min interval)")

    yield

    # å…³é—­æ—¶çš„æ“ä½œ
    if scheduler:
        scheduler.shutdown()
        logger.info("â° Scheduler shutdown")
    logger.info("ğŸ›‘ API Server shutting down...")


app = FastAPI(
    title="Gemini API Proxy",
    description="A high-performance proxy for Gemini API with OpenAI compatibility",
    version="1.0.0",
    lifespan=lifespan
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒåº”é™åˆ¶å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# è¯·æ±‚è®¡æ•°ä¸­é—´ä»¶
@app.middleware("http")
async def count_requests(request: Request, call_next):
    global request_count
    request_count += 1

    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time

    # è®°å½•è¯·æ±‚æ—¥å¿—
    logger.info(f"ğŸ“¡ {request.method} {request.url.path} - {response.status_code} - {process_time:.3f}s")

    return response


# å…¨å±€å¼‚å¸¸å¤„ç†å™¨
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    """å¤„ç†è¯·æ±‚éªŒè¯é”™è¯¯"""
    logger.warning(f"âŒ Request validation error: {exc}")

    # æå–å…·ä½“çš„é”™è¯¯ä¿¡æ¯
    error_details = []
    for error in exc.errors():
        field = " -> ".join(str(x) for x in error["loc"])
        msg = error["msg"]
        error_details.append(f"{field}: {msg}")

    return JSONResponse(
        status_code=422,
        content={
            "error": {
                "message": f"Request validation failed: {'; '.join(error_details)}",
                "type": "invalid_request_error",
                "code": "request_validation_error"
            }
        }
    )


@app.exception_handler(ValidationError)
async def pydantic_validation_exception_handler(request: Request, exc: ValidationError):
    """å¤„ç†PydanticéªŒè¯é”™è¯¯"""
    logger.warning(f"âŒ Pydantic validation error: {exc}")

    return JSONResponse(
        status_code=422,
        content={
            "error": {
                "message": f"Data validation failed: {str(exc)}",
                "type": "invalid_request_error",
                "code": "data_validation_error"
            }
        }
    )


@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """å…¨å±€å¼‚å¸¸å¤„ç†"""
    logger.error(f"ğŸ’¥ Global exception: {str(exc)}")
    return JSONResponse(
        status_code=500,
        content={
            "error": {
                "message": "Internal server error",
                "type": "internal_error",
                "code": "server_error"
            }
        }
    )


# è¾…åŠ©å‡½æ•°
def get_actual_model_name(request_model: str) -> str:
    """è·å–å®é™…ä½¿ç”¨çš„æ¨¡å‹åç§°"""
    supported_models = db.get_supported_models()

    # å¦‚æœè¯·æ±‚çš„æ˜¯æ”¯æŒçš„æ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨
    if request_model in supported_models:
        logger.info(f"âœ… Using requested model: {request_model}")
        return request_model

    # å¦åˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
    default_model = db.get_config('default_model_name', 'gemini-2.5-flash')
    logger.info(f"âš ï¸ Unsupported model: {request_model}, using default: {default_model}")
    return default_model


def inject_prompt_to_messages(messages: List[ChatMessage]) -> List[ChatMessage]:
    """å‘æ¶ˆæ¯ä¸­æ³¨å…¥prompt"""
    inject_config = db.get_inject_prompt_config()

    if not inject_config['enabled'] or not inject_config['content']:
        return messages

    content = inject_config['content']
    position = inject_config['position']

    # åˆ›å»ºæ¶ˆæ¯å‰¯æœ¬
    new_messages = messages.copy()

    if position == 'system':
        # æ·»åŠ æˆ–æ›´æ–°systemæ¶ˆæ¯
        system_msg = None
        for i, msg in enumerate(new_messages):
            if msg.role == 'system':
                system_msg = msg
                break

        if system_msg:
            # æ›´æ–°ç°æœ‰systemæ¶ˆæ¯
            new_content = f"{content}\n\n{system_msg.get_text_content()}"
            new_messages[i] = ChatMessage(role='system', content=new_content)
        else:
            # æ·»åŠ æ–°çš„systemæ¶ˆæ¯åˆ°å¼€å¤´
            new_messages.insert(0, ChatMessage(role='system', content=content))

    elif position == 'user_prefix':
        # åœ¨ç¬¬ä¸€ä¸ªuseræ¶ˆæ¯å‰æ·»åŠ 
        for i, msg in enumerate(new_messages):
            if msg.role == 'user':
                original_content = msg.get_text_content()
                new_content = f"{content}\n\n{original_content}"
                new_messages[i] = ChatMessage(role='user', content=new_content)
                break

    elif position == 'user_suffix':
        # åœ¨æœ€åä¸€ä¸ªuseræ¶ˆæ¯åæ·»åŠ 
        for i in range(len(new_messages) - 1, -1, -1):
            if new_messages[i].role == 'user':
                original_content = new_messages[i].get_text_content()
                new_content = f"{original_content}\n\n{content}"
                new_messages[i] = ChatMessage(role='user', content=new_content)
                break

    return new_messages


def get_thinking_config(request: ChatCompletionRequest) -> Dict:
    """æ ¹æ®é…ç½®ç”Ÿæˆæ€è€ƒé…ç½®"""
    thinking_config = {}

    # ä»æ•°æ®åº“è·å–å…¨å±€é…ç½®
    global_thinking_enabled = db.get_config('thinking_enabled', 'true').lower() == 'true'
    global_thinking_budget = int(db.get_config('thinking_budget', '-1'))  # -1 è¡¨ç¤ºè‡ªåŠ¨
    global_include_thoughts = db.get_config('include_thoughts', 'false').lower() == 'true'

    # å¦‚æœå…¨å±€ç¦ç”¨æ€è€ƒï¼Œç›´æ¥è¿”å›ç¦ç”¨é…ç½®
    if not global_thinking_enabled:
        return {"thinkingBudget": 0}

    # å¦‚æœè¯·æ±‚ä¸­æœ‰æ€è€ƒé…ç½®ï¼Œä¼˜å…ˆä½¿ç”¨è¯·æ±‚çš„é…ç½®
    if request.thinking_config:
        if request.thinking_config.thinking_budget is not None:
            thinking_config["thinkingBudget"] = request.thinking_config.thinking_budget
        elif global_thinking_budget >= 0:
            thinking_config["thinkingBudget"] = global_thinking_budget

        if request.thinking_config.include_thoughts is not None:
            thinking_config["includeThoughts"] = request.thinking_config.include_thoughts
        elif global_include_thoughts:
            thinking_config["includeThoughts"] = global_include_thoughts
    else:
        # ä½¿ç”¨å…¨å±€é…ç½®
        if global_thinking_budget >= 0:
            thinking_config["thinkingBudget"] = global_thinking_budget
        if global_include_thoughts:
            thinking_config["includeThoughts"] = global_include_thoughts

    return thinking_config


def openai_to_gemini(request: ChatCompletionRequest) -> Dict:
    """å°†OpenAIæ ¼å¼è½¬æ¢ä¸ºGeminiæ ¼å¼"""
    contents = []

    for msg in request.messages:
        # ç¡®ä¿è·å–æ–‡æœ¬å†…å®¹
        text_content = msg.get_text_content()

        if msg.role == "system":
            # Geminiæ²¡æœ‰systemè§’è‰²ï¼Œå°†å…¶è½¬æ¢ä¸ºuseræ¶ˆæ¯
            contents.append({
                "role": "user",
                "parts": [{"text": f"[System]: {text_content}"}]
            })
        elif msg.role == "user":
            contents.append({
                "role": "user",
                "parts": [{"text": text_content}]
            })
        elif msg.role == "assistant":
            contents.append({
                "role": "model",
                "parts": [{"text": text_content}]
            })

    # æ„å»ºåŸºæœ¬è¯·æ±‚
    gemini_request = {
        "contents": contents,
        "generationConfig": {
            "temperature": request.temperature,
            "topP": request.top_p,
            "candidateCount": request.n,
        }
    }

    # æ·»åŠ æ€è€ƒé…ç½®
    thinking_config = get_thinking_config(request)
    if thinking_config:
        gemini_request["generationConfig"]["thinkingConfig"] = thinking_config

    # æ·»åŠ å…¶ä»–å‚æ•°
    if request.max_tokens:
        gemini_request["generationConfig"]["maxOutputTokens"] = request.max_tokens

    if request.stop:
        gemini_request["generationConfig"]["stopSequences"] = request.stop

    return gemini_request


def extract_thoughts_and_content(gemini_response: Dict) -> tuple[str, str]:
    """ä»Geminiå“åº”ä¸­æå–æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆå†…å®¹"""
    thoughts = ""
    content = ""

    for candidate in gemini_response.get("candidates", []):
        parts = candidate.get("content", {}).get("parts", [])

        for part in parts:
            if "text" in part:
                # æ£€æŸ¥æ˜¯å¦ä¸ºæ€è€ƒéƒ¨åˆ†
                if part.get("thought", False):
                    thoughts += part["text"]
                else:
                    content += part["text"]

    return thoughts, content


def gemini_to_openai(gemini_response: Dict, request: ChatCompletionRequest, usage_info: Dict = None) -> Dict:
    """å°†Geminiå“åº”è½¬æ¢ä¸ºOpenAIæ ¼å¼"""
    choices = []

    # æå–æ€è€ƒè¿‡ç¨‹å’Œå†…å®¹
    thoughts, content = extract_thoughts_and_content(gemini_response)

    for i, candidate in enumerate(gemini_response.get("candidates", [])):
        # æ„å»ºå“åº”æ¶ˆæ¯
        message_content = content if content else ""

        # å¦‚æœæœ‰æ€è€ƒè¿‡ç¨‹ä¸”é…ç½®è¦æ±‚åŒ…å«ï¼Œæ·»åŠ åˆ°å“åº”ä¸­
        if thoughts and request.thinking_config and request.thinking_config.include_thoughts:
            message_content = f"**Thinking:**\n{thoughts}\n\n**Response:**\n{content}"

        choices.append({
            "index": i,
            "message": {
                "role": "assistant",
                "content": message_content
            },
            "finish_reason": map_finish_reason(candidate.get("finishReason", "STOP"))
        })

    # æ„å»ºå“åº”
    response = {
        "id": f"chatcmpl-{uuid.uuid4().hex[:8]}",
        "object": "chat.completion",
        "created": int(time.time()),
        "model": request.model,
        "choices": choices,
        "usage": usage_info or {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0
        }
    }

    return response


def map_finish_reason(gemini_reason: str) -> str:
    """æ˜ å°„Geminiçš„ç»“æŸåŸå› åˆ°OpenAIæ ¼å¼"""
    mapping = {
        "STOP": "stop",
        "MAX_TOKENS": "length",
        "SAFETY": "content_filter",
        "RECITATION": "content_filter",
        "OTHER": "stop"
    }
    return mapping.get(gemini_reason, "stop")


async def select_gemini_key_and_check_limits(model_name: str) -> Optional[Dict]:
    """é€‰æ‹©å¯ç”¨çš„Gemini Keyå¹¶æ£€æŸ¥æ¨¡å‹é™åˆ¶"""
    available_keys = db.get_available_gemini_keys()

    if not available_keys:
        logger.warning("âš ï¸ No available Gemini keys found")
        return None

    # è·å–æ¨¡å‹é…ç½®ï¼ˆå·²ç»åŒ…å«è®¡ç®—çš„æ€»é™åˆ¶ï¼‰
    model_config = db.get_model_config(model_name)
    if not model_config:
        logger.error(f"âŒ Model config not found for: {model_name}")
        return None

    # è®°å½•é™åˆ¶ä¿¡æ¯
    logger.info(
        f"ğŸ“ˆ Model {model_name} limits: RPM={model_config['total_rpm_limit']}, TPM={model_config['total_tpm_limit']}, RPD={model_config['total_rpd_limit']}")
    logger.info(f"ğŸ”‘ Available API keys: {len(available_keys)}")

    # æ£€æŸ¥æ¨¡å‹çº§åˆ«çš„é™åˆ¶ï¼ˆä½¿ç”¨æ€»é™åˆ¶ï¼‰
    current_usage = await rate_limiter.get_current_usage(model_name)

    if (current_usage['requests'] >= model_config['total_rpm_limit'] or
            current_usage['tokens'] >= model_config['total_tpm_limit']):
        logger.warning(
            f"ğŸš« Model {model_name} has reached rate limits: requests={current_usage['requests']}/{model_config['total_rpm_limit']}, tokens={current_usage['tokens']}/{model_config['total_tpm_limit']}")
        return None

    # æ£€æŸ¥RPDé™åˆ¶ï¼ˆä½¿ç”¨æ€»é™åˆ¶ï¼‰
    day_usage = db.get_usage_stats(model_name, 'day')
    if day_usage['requests'] >= model_config['total_rpd_limit']:
        logger.warning(
            f"ğŸš« Model {model_name} has reached daily request limit: {day_usage['requests']}/{model_config['total_rpd_limit']}")
        return None

    # è´Ÿè½½å‡è¡¡ç­–ç•¥é€‰æ‹©Key
    strategy = db.get_config('load_balance_strategy', 'least_used')

    if strategy == 'round_robin':
        # ç®€å•è½®è¯¢ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„
        selected_key = available_keys[0]
    else:
        # least_usedç­–ç•¥ï¼Œé€‰æ‹©æœ€å°‘ä½¿ç”¨çš„Keyï¼ˆåŸºäºKey IDçš„ä½¿ç”¨åˆ†å¸ƒï¼‰
        selected_key = available_keys[0]

    logger.info(f"ğŸ¯ Selected API key #{selected_key['id']} for model {model_name}")

    # è¿”å›é€‰ä¸­çš„Keyå’Œæ¨¡å‹é…ç½®
    return {
        'key_info': selected_key,
        'model_config': model_config
    }


async def make_gemini_request_with_retry(
        gemini_key: str,
        gemini_request: Dict,
        model_name: str,
        max_retries: int = 3
) -> Dict:
    """å¸¦é‡è¯•çš„Gemini APIè¯·æ±‚"""
    timeout = float(db.get_config('request_timeout', '60'))

    for attempt in range(max_retries):
        try:
            async with httpx.AsyncClient(timeout=timeout) as client:
                gemini_url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:generateContent"

                response = await client.post(
                    gemini_url,
                    json=gemini_request,
                    headers={"x-goog-api-key": gemini_key}
                )

                if response.status_code == 200:
                    return response.json()
                else:
                    error_detail = response.json() if response.content else {"error": {"message": "Unknown error"}}
                    if attempt == max_retries - 1:  # æœ€åä¸€æ¬¡å°è¯•
                        raise HTTPException(
                            status_code=response.status_code,
                            detail=error_detail.get("error", {}).get("message", "Unknown error")
                        )
                    else:
                        logger.warning(f"ğŸ”„ Request failed (attempt {attempt + 1}), retrying...")
                        await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                        continue

        except httpx.TimeoutException as e:
            if attempt == max_retries - 1:
                raise HTTPException(status_code=504, detail="Request timeout")
            else:
                logger.warning(f"â° Request timeout (attempt {attempt + 1}), retrying...")
                await asyncio.sleep(2 ** attempt)
                continue
        except Exception as e:
            if attempt == max_retries - 1:
                raise HTTPException(status_code=500, detail=str(e))
            else:
                logger.warning(f"âŒ Request failed (attempt {attempt + 1}): {str(e)}, retrying...")
                await asyncio.sleep(2 ** attempt)
                continue

    raise HTTPException(status_code=500, detail="Max retries exceeded")


async def stream_gemini_response(
        gemini_key: str,
        gemini_request: Dict,
        openai_request: ChatCompletionRequest,
        key_info: Dict,
        model_name: str
) -> AsyncGenerator[str, None]:
    """å¤„ç†Geminiçš„æµå¼å“åº”"""
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ·»åŠ  alt=sse å‚æ•°ï¼Œè¿™æ˜¯å®˜æ–¹æ–‡æ¡£è¦æ±‚çš„
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:streamGenerateContent?alt=sse"
    timeout = float(db.get_config('request_timeout', '60'))
    max_retries = int(db.get_config('max_retries', '3'))

    logger.info(f"ğŸŒŠ Starting stream request to: {url}")

    for attempt in range(max_retries):
        try:
            async with httpx.AsyncClient(timeout=timeout) as client:
                async with client.stream(
                        "POST",
                        url,
                        json=gemini_request,
                        headers={"x-goog-api-key": gemini_key}
                ) as response:
                    if response.status_code != 200:
                        error_text = await response.aread()
                        error_msg = error_text.decode() if error_text else "Unknown error"
                        logger.error(f"âŒ Stream request failed with status {response.status_code}: {error_msg}")
                        yield f"data: {json.dumps({'error': {'message': error_msg, 'type': 'api_error', 'code': response.status_code}})}\n\n"
                        yield "data: [DONE]\n\n"
                        return

                    stream_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"
                    created = int(time.time())
                    total_tokens = 0
                    thinking_sent = False
                    has_content = False
                    processed_lines = 0

                    logger.info(f"âœ… Stream response started, status: {response.status_code}")

                    try:
                        # ğŸ”¥ æŒ‰ç…§å®˜æ–¹æ–‡æ¡£çš„SSEæ ¼å¼å¤„ç†
                        async for line in response.aiter_lines():
                            processed_lines += 1

                            if not line:
                                continue

                            # è®°å½•åŸå§‹è¡Œä»¥ä¾¿è°ƒè¯•
                            if processed_lines <= 5:  # åªè®°å½•å‰å‡ è¡Œ
                                logger.debug(f"ğŸ“ Stream line {processed_lines}: {line[:100]}...")

                            # å®˜æ–¹SSEæ ¼å¼ï¼šæ¯è¡Œä»¥ "data: " å¼€å¤´
                            if line.startswith("data: "):
                                json_str = line[6:]  # å»æ‰ "data: " å‰ç¼€

                                # æ£€æŸ¥ç»“æŸæ ‡å¿—
                                if json_str.strip() == "[DONE]":
                                    logger.info("ğŸ Received [DONE] signal from stream")
                                    break

                                if not json_str.strip():
                                    continue

                                try:
                                    data = json.loads(json_str)

                                    # å¤„ç†å€™é€‰å“åº”
                                    for candidate in data.get("candidates", []):
                                        content_data = candidate.get("content", {})
                                        parts = content_data.get("parts", [])

                                        for part in parts:
                                            if "text" in part:
                                                text = part["text"]
                                                if not text:  # è·³è¿‡ç©ºæ–‡æœ¬
                                                    continue

                                                total_tokens += len(text.split())
                                                has_content = True

                                                # æ£€æŸ¥æ˜¯å¦ä¸ºæ€è€ƒéƒ¨åˆ†
                                                is_thought = part.get("thought", False)

                                                # å¦‚æœæ˜¯æ€è€ƒéƒ¨åˆ†ä¸”é…ç½®ä¸åŒ…å«æ€è€ƒï¼Œè·³è¿‡
                                                if is_thought and not (openai_request.thinking_config and
                                                                       openai_request.thinking_config.include_thoughts):
                                                    continue

                                                # æ€è€ƒåŠŸèƒ½çš„å¤„ç†é€»è¾‘
                                                if is_thought and not thinking_sent:
                                                    thinking_header = {
                                                        "id": stream_id,
                                                        "object": "chat.completion.chunk",
                                                        "created": created,
                                                        "model": openai_request.model,
                                                        "choices": [{
                                                            "index": 0,
                                                            "delta": {"content": "**Thinking:**\n"},
                                                            "finish_reason": None
                                                        }]
                                                    }
                                                    yield f"data: {json.dumps(thinking_header)}\n\n"
                                                    thinking_sent = True
                                                    logger.debug("ğŸ§  Sent thinking header")
                                                elif not is_thought and thinking_sent:
                                                    response_header = {
                                                        "id": stream_id,
                                                        "object": "chat.completion.chunk",
                                                        "created": created,
                                                        "model": openai_request.model,
                                                        "choices": [{
                                                            "index": 0,
                                                            "delta": {"content": "\n\n**Response:**\n"},
                                                            "finish_reason": None
                                                        }]
                                                    }
                                                    yield f"data: {json.dumps(response_header)}\n\n"
                                                    thinking_sent = False
                                                    logger.debug("ğŸ’¬ Sent response header")

                                                # å‘é€æ–‡æœ¬å†…å®¹
                                                chunk_data = {
                                                    "id": stream_id,
                                                    "object": "chat.completion.chunk",
                                                    "created": created,
                                                    "model": openai_request.model,
                                                    "choices": [{
                                                        "index": 0,
                                                        "delta": {"content": text},
                                                        "finish_reason": None
                                                    }]
                                                }
                                                yield f"data: {json.dumps(chunk_data)}\n\n"

                                        # æ£€æŸ¥æ˜¯å¦ç»“æŸ
                                        finish_reason = candidate.get("finishReason")
                                        if finish_reason:
                                            finish_chunk = {
                                                "id": stream_id,
                                                "object": "chat.completion.chunk",
                                                "created": created,
                                                "model": openai_request.model,
                                                "choices": [{
                                                    "index": 0,
                                                    "delta": {},
                                                    "finish_reason": map_finish_reason(finish_reason)
                                                }]
                                            }
                                            yield f"data: {json.dumps(finish_chunk)}\n\n"
                                            yield "data: [DONE]\n\n"

                                            logger.info(
                                                f"âœ… Stream completed with finish_reason: {finish_reason}, tokens: {total_tokens}")

                                            # è®°å½•ä½¿ç”¨é‡
                                            await rate_limiter.add_usage(model_name, 1, total_tokens)
                                            return

                                except json.JSONDecodeError as e:
                                    logger.warning(f"âš ï¸ JSON decode error: {e}, line: {json_str[:200]}...")
                                    continue

                            # å¤„ç†å…¶ä»–SSEäº‹ä»¶ç±»å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
                            elif line.startswith("event: "):
                                # å¯ä»¥å¤„ç†ç‰¹å®šäº‹ä»¶ç±»å‹
                                continue
                            elif line.startswith("id: ") or line.startswith("retry: "):
                                # SSEå…ƒæ•°æ®ï¼Œå¿½ç•¥
                                continue

                        # å¦‚æœæµæ­£å¸¸ç»“æŸä½†æ²¡æœ‰æ˜¾å¼çš„ç»“æŸä¿¡å·
                        if has_content:
                            finish_chunk = {
                                "id": stream_id,
                                "object": "chat.completion.chunk",
                                "created": created,
                                "model": openai_request.model,
                                "choices": [{
                                    "index": 0,
                                    "delta": {},
                                    "finish_reason": "stop"
                                }]
                            }
                            yield f"data: {json.dumps(finish_chunk)}\n\n"
                            yield "data: [DONE]\n\n"

                            logger.info(
                                f"âœ… Stream ended naturally, processed {processed_lines} lines, tokens: {total_tokens}")

                        # å¦‚æœç¡®å®æ²¡æœ‰æ”¶åˆ°ä»»ä½•å†…å®¹ï¼Œæ‰å›é€€
                        if not has_content:
                            logger.warning(
                                f"âš ï¸ Stream response had no content after processing {processed_lines} lines, falling back to non-stream")
                            try:
                                fallback_response = await make_gemini_request_with_retry(
                                    gemini_key, gemini_request, model_name, 1
                                )

                                thoughts, content = extract_thoughts_and_content(fallback_response)

                                if thoughts and openai_request.thinking_config and openai_request.thinking_config.include_thoughts:
                                    full_content = f"**Thinking:**\n{thoughts}\n\n**Response:**\n{content}"
                                else:
                                    full_content = content

                                if full_content:
                                    chunk_data = {
                                        "id": stream_id,
                                        "object": "chat.completion.chunk",
                                        "created": created,
                                        "model": openai_request.model,
                                        "choices": [{
                                            "index": 0,
                                            "delta": {"content": full_content},
                                            "finish_reason": None
                                        }]
                                    }
                                    yield f"data: {json.dumps(chunk_data)}\n\n"

                                    finish_chunk = {
                                        "id": stream_id,
                                        "object": "chat.completion.chunk",
                                        "created": created,
                                        "model": openai_request.model,
                                        "choices": [{
                                            "index": 0,
                                            "delta": {},
                                            "finish_reason": "stop"
                                        }]
                                    }
                                    yield f"data: {json.dumps(finish_chunk)}\n\n"
                                    total_tokens = len(full_content.split())

                                    logger.info(f"âœ… Fallback completed, tokens: {total_tokens}")

                            except Exception as e:
                                logger.error(f"âŒ Fallback request failed: {e}")
                                yield f"data: {json.dumps({'error': {'message': 'Failed to get response', 'type': 'server_error'}})}\n\n"

                        # è®°å½•ä½¿ç”¨é‡
                        await rate_limiter.add_usage(model_name, 1, total_tokens)
                        yield "data: [DONE]\n\n"
                        return  # æˆåŠŸå®Œæˆ

                    except (httpx.ReadError, httpx.RemoteProtocolError) as e:
                        logger.warning(f"ğŸ”Œ Stream connection error (attempt {attempt + 1}): {str(e)}")
                        if attempt < max_retries - 1:
                            yield f"data: {json.dumps({'error': {'message': 'Connection interrupted, retrying...', 'type': 'connection_error'}})}\n\n"
                            await asyncio.sleep(1)
                            continue
                        else:
                            yield f"data: {json.dumps({'error': {'message': 'Stream connection failed after retries', 'type': 'connection_error'}})}\n\n"
                            yield "data: [DONE]\n\n"
                            return

        except (httpx.TimeoutException, httpx.ConnectError) as e:
            logger.warning(f"â° Connection error (attempt {attempt + 1}): {str(e)}")
            if attempt < max_retries - 1:
                yield f"data: {json.dumps({'error': {'message': f'Connection error, retrying... (attempt {attempt + 1})', 'type': 'connection_error'}})}\n\n"
                await asyncio.sleep(2 ** attempt)
                continue
            else:
                yield f"data: {json.dumps({'error': {'message': 'Connection failed after all retries', 'type': 'connection_error'}})}\n\n"
                yield "data: [DONE]\n\n"
                return
        except Exception as e:
            logger.error(f"ğŸ’¥ Unexpected error in stream (attempt {attempt + 1}): {str(e)}")
            if attempt < max_retries - 1:
                await asyncio.sleep(1)
                continue
            else:
                yield f"data: {json.dumps({'error': {'message': 'Unexpected error occurred', 'type': 'server_error'}})}\n\n"
                yield "data: [DONE]\n\n"
                return


# APIç«¯ç‚¹
@app.get("/")
async def root():
    """æ ¹ç«¯ç‚¹"""
    return {
        "service": "Gemini API Proxy",
        "status": "running",
        "version": "1.0.0",
        "docs": "/docs",
        "health": "/health"
    }


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    available_keys = len(db.get_available_gemini_keys())
    uptime = time.time() - start_time

    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "available_keys": available_keys,
        "environment": "render" if os.getenv('RENDER_EXTERNAL_URL') else "local",
        "uptime_seconds": int(uptime),
        "request_count": request_count,
        "version": "1.0.0"
    }


@app.get("/wake")
async def wake_up():
    """å¿«é€Ÿå”¤é†’ç«¯ç‚¹"""
    return {
        "status": "awake",
        "timestamp": datetime.now().isoformat(),
        "message": "Service is active"
    }


@app.get("/status")
async def get_status():
    """è·å–è¯¦ç»†æœåŠ¡çŠ¶æ€"""
    import psutil
    import sys

    process = psutil.Process(os.getpid())

    return {
        "service": "Gemini API Proxy",
        "status": "running",
        "version": "1.0.0",
        "render_url": os.getenv('RENDER_EXTERNAL_URL'),
        "python_version": sys.version,
        "models": db.get_supported_models(),
        "active_keys": len(db.get_available_gemini_keys()),
        "memory_usage_mb": process.memory_info().rss / 1024 / 1024,
        "cpu_percent": process.cpu_percent(),
        "uptime_seconds": int(time.time() - start_time),
        "total_requests": request_count,
        "thinking_enabled": db.get_thinking_config()['enabled'],
        "keep_alive_active": scheduler is not None and scheduler.running
    }


@app.get("/metrics")
async def get_metrics():
    """è·å–æœåŠ¡æŒ‡æ ‡"""
    import psutil

    process = psutil.Process(os.getpid())

    return {
        "memory_usage_mb": process.memory_info().rss / 1024 / 1024,
        "cpu_percent": process.cpu_percent(),
        "active_connections": len(db.get_available_gemini_keys()),
        "uptime_seconds": int(time.time() - start_time),
        "requests_count": request_count,
        "database_size_mb": os.path.getsize(db.db_path) / 1024 / 1024 if os.path.exists(db.db_path) else 0
    }


@app.post("/v1/chat/completions")
async def chat_completions(
        request: ChatCompletionRequest,
        authorization: str = Header(None)
):
    try:
        # éªŒè¯API Key
        if not authorization or not authorization.startswith("Bearer "):
            raise HTTPException(status_code=401, detail="Invalid authorization header")

        api_key = authorization.replace("Bearer ", "")
        user_key = db.validate_user_key(api_key)

        if not user_key:
            raise HTTPException(status_code=401, detail="Invalid API key")

        # åŸºç¡€è¯·æ±‚éªŒè¯
        if not request.messages or len(request.messages) == 0:
            raise HTTPException(status_code=422, detail="Messages cannot be empty")

        # éªŒè¯æ¶ˆæ¯æ ¼å¼
        for msg in request.messages:
            if not hasattr(msg, 'role') or not hasattr(msg, 'content'):
                raise HTTPException(status_code=422, detail="Invalid message format")
            if msg.role not in ['system', 'user', 'assistant']:
                raise HTTPException(status_code=422, detail=f"Invalid role: {msg.role}")

            # ç¡®ä¿contentå·²ç»è¢«æ ‡å‡†åŒ–ä¸ºå­—ç¬¦ä¸²
            if not isinstance(msg.content, str):
                raise HTTPException(status_code=422, detail="Content validation failed")

        # è·å–å®é™…ä½¿ç”¨çš„æ¨¡å‹åç§°
        actual_model_name = get_actual_model_name(request.model)

        # æ³¨å…¥promptï¼ˆå¦‚æœå¯ç”¨ï¼‰
        request.messages = inject_prompt_to_messages(request.messages)

        # é€‰æ‹©å¯ç”¨çš„Gemini Keyå¹¶æ£€æŸ¥é™åˆ¶
        selection_result = await select_gemini_key_and_check_limits(actual_model_name)

        if not selection_result:
            raise HTTPException(
                status_code=429,
                detail=f"No available API keys or model {actual_model_name} has reached rate limits."
            )

        gemini_key_info = selection_result['key_info']
        model_config = selection_result['model_config']

        # è½¬æ¢è¯·æ±‚æ ¼å¼
        gemini_request = openai_to_gemini(request)

        # è®°å½•è¯·æ±‚
        await rate_limiter.add_usage(actual_model_name, 1, 0)
        db.log_usage(gemini_key_info['id'], user_key['id'], actual_model_name, 1, 0)

        if request.stream:
            # æµå¼å“åº”
            return StreamingResponse(
                stream_gemini_response(
                    gemini_key_info['key'],
                    gemini_request,
                    request,
                    gemini_key_info,
                    actual_model_name
                ),
                media_type="text/event-stream"
            )
        else:
            # éæµå¼å“åº”
            gemini_response = await make_gemini_request_with_retry(
                gemini_key_info['key'],
                gemini_request,
                actual_model_name,
                int(db.get_config('max_retries', '3'))
            )

            # ä¼°ç®—tokenä½¿ç”¨é‡
            total_tokens = 0
            for candidate in gemini_response.get("candidates", []):
                content = candidate.get("content", {})
                parts = content.get("parts", [])
                for part in parts:
                    if "text" in part:
                        total_tokens += len(part["text"].split())

            # è®°å½•tokenä½¿ç”¨
            await rate_limiter.add_usage(actual_model_name, 0, total_tokens)
            db.log_usage(gemini_key_info['id'], user_key['id'], actual_model_name, 0, total_tokens)

            # è½¬æ¢å“åº”æ ¼å¼
            usage_info = {
                "prompt_tokens": len(str(request.messages).split()),
                "completion_tokens": total_tokens,
                "total_tokens": len(str(request.messages).split()) + total_tokens
            }

            openai_response = gemini_to_openai(gemini_response, request, usage_info)

            return JSONResponse(content=openai_response)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ğŸ’¥ Unexpected error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/v1/models")
async def list_models():
    """åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹"""
    models = db.get_supported_models()

    model_list = []
    for model in models:
        model_list.append({
            "id": model,
            "object": "model",
            "created": int(time.time()),
            "owned_by": "google"
        })

    return {"object": "list", "data": model_list}


# ğŸ”¥ æ–°å¢ï¼šç®¡ç†ç«¯ç‚¹ï¼ˆä¿®å¤404é”™è¯¯ï¼‰
@app.get("/admin/models/{model_name}")
async def get_model_config(model_name: str):
    """è·å–æŒ‡å®šæ¨¡å‹çš„é…ç½®"""
    try:
        model_config = db.get_model_config(model_name)
        if not model_config:
            raise HTTPException(status_code=404, detail=f"Model {model_name} not found")

        return {
            "success": True,
            "model_name": model_name,
            **model_config
        }
    except Exception as e:
        logger.error(f"âŒ Failed to get model config for {model_name}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/admin/models/{model_name}")
async def update_model_config(model_name: str, request: dict):
    """æ›´æ–°æŒ‡å®šæ¨¡å‹çš„é…ç½®"""
    try:
        # éªŒè¯æ¨¡å‹æ˜¯å¦å­˜åœ¨
        if model_name not in db.get_supported_models():
            raise HTTPException(status_code=404, detail=f"Model {model_name} not supported")

        # æå–å…è®¸æ›´æ–°çš„å­—æ®µ
        allowed_fields = ['single_api_rpm_limit', 'single_api_tpm_limit', 'single_api_rpd_limit', 'status']
        update_data = {}

        for field in allowed_fields:
            if field in request:
                update_data[field] = request[field]

        if not update_data:
            raise HTTPException(status_code=422, detail="No valid fields to update")

        # æ›´æ–°æ¨¡å‹é…ç½®
        success = db.update_model_config(model_name, **update_data)

        if success:
            logger.info(f"âœ… Updated model config for {model_name}: {update_data}")
            return {
                "success": True,
                "message": f"Model {model_name} configuration updated successfully",
                "updated_fields": update_data
            }
        else:
            raise HTTPException(status_code=500, detail="Failed to update model configuration")

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Failed to update model config for {model_name}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/admin/models")
async def list_model_configs():
    """è·å–æ‰€æœ‰æ¨¡å‹çš„é…ç½®"""
    try:
        model_configs = db.get_all_model_configs()
        return {
            "success": True,
            "models": model_configs
        }
    except Exception as e:
        logger.error(f"âŒ Failed to get model configs: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/admin/config/gemini-key")
async def add_gemini_key(request: dict):
    """é€šè¿‡APIæ·»åŠ Geminiå¯†é’¥"""
    key = request.get("key")
    if key and db.add_gemini_key(key):
        logger.info(f"âœ… Added new Gemini API key")
        return {"success": True, "message": "Key added successfully"}
    return {"success": False, "message": "Failed to add key"}


@app.post("/admin/config/user-key")
async def generate_user_key(request: dict):
    """ç”Ÿæˆç”¨æˆ·å¯†é’¥"""
    name = request.get("name", "API User")
    key = db.generate_user_key(name)
    logger.info(f"ğŸ”‘ Generated new user key for: {name}")
    return {"success": True, "key": key, "name": name}


@app.post("/admin/config/thinking")
async def update_thinking_config(request: dict):
    """æ›´æ–°æ€è€ƒæ¨¡å¼é…ç½®"""
    try:
        enabled = request.get('enabled')
        budget = request.get('budget')
        include_thoughts = request.get('include_thoughts')

        success = db.set_thinking_config(
            enabled=enabled,
            budget=budget,
            include_thoughts=include_thoughts
        )

        if success:
            logger.info(f"âœ… Updated thinking config: {request}")
            return {
                "success": True,
                "message": "Thinking configuration updated successfully"
            }
        else:
            raise HTTPException(status_code=500, detail="Failed to update thinking configuration")

    except ValueError as e:
        raise HTTPException(status_code=422, detail=str(e))
    except Exception as e:
        logger.error(f"âŒ Failed to update thinking config: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/admin/config/inject-prompt")
async def update_inject_prompt_config(request: dict):
    """æ›´æ–°æç¤ºè¯æ³¨å…¥é…ç½®"""
    try:
        enabled = request.get('enabled')
        content = request.get('content')
        position = request.get('position')

        success = db.set_inject_prompt_config(
            enabled=enabled,
            content=content,
            position=position
        )

        if success:
            logger.info(f"âœ… Updated inject prompt config: enabled={enabled}, position={position}")
            return {
                "success": True,
                "message": "Inject prompt configuration updated successfully"
            }
        else:
            raise HTTPException(status_code=500, detail="Failed to update inject prompt configuration")

    except ValueError as e:
        raise HTTPException(status_code=422, detail=str(e))
    except Exception as e:
        logger.error(f"âŒ Failed to update inject prompt config: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/admin/config")
async def get_all_config():
    """è·å–æ‰€æœ‰ç³»ç»Ÿé…ç½®"""
    try:
        configs = db.get_all_configs()
        thinking_config = db.get_thinking_config()
        inject_config = db.get_inject_prompt_config()

        return {
            "success": True,
            "system_configs": configs,
            "thinking_config": thinking_config,
            "inject_config": inject_config
        }
    except Exception as e:
        logger.error(f"âŒ Failed to get configs: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/admin/stats")
async def get_admin_stats():
    """è·å–ç®¡ç†ç»Ÿè®¡"""
    return {
        "gemini_keys": len(db.get_all_gemini_keys()),
        "active_gemini_keys": len(db.get_available_gemini_keys()),
        "user_keys": len(db.get_all_user_keys()),
        "active_user_keys": len([k for k in db.get_all_user_keys() if k['status'] == 1]),
        "supported_models": db.get_supported_models(),
        "usage_stats": db.get_all_usage_stats(),
        "thinking_config": db.get_thinking_config(),
        "inject_config": db.get_inject_prompt_config()
    }


# è¿è¡ŒæœåŠ¡å™¨çš„å‡½æ•°
def run_api_server(port: int = 8000):
    """è¿è¡ŒAPIæœåŠ¡å™¨"""
    uvicorn.run(app, host="0.0.0.0", port=port, log_level="info")


if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    logger.info(f"ğŸš€ Starting Gemini API Proxy on port {port}")
    run_api_server(port)